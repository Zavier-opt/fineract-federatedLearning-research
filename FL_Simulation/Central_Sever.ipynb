{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69ada23f",
   "metadata": {},
   "source": [
    "## Join the Duet Server the Data Owner 1 connected to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d6dfd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e78b315c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤  ðŸŽ¸  â™ªâ™ªâ™ª Joining Duet â™«â™«â™«  ðŸŽ»  ðŸŽ¹\n",
      "\n",
      "â™«â™«â™« >\u001b[93m DISCLAIMER\u001b[0m: \u001b[1mDuet is an experimental feature currently in beta.\n",
      "â™«â™«â™« > Use at your own risk.\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "    > â¤ï¸ \u001b[91mLove\u001b[0m \u001b[92mDuet\u001b[0m? \u001b[93mPlease\u001b[0m \u001b[94mconsider\u001b[0m \u001b[95msupporting\u001b[0m \u001b[91mour\u001b[0m \u001b[93mcommunity!\u001b[0m\n",
      "    > https://github.com/sponsors/OpenMined\u001b[1m\n",
      "\n",
      "â™«â™«â™« > Punching through firewall to OpenGrid Network Node at:\n",
      "â™«â™«â™« > http://ec2-18-218-7-180.us-east-2.compute.amazonaws.com:5000\n",
      "â™«â™«â™« >\n",
      "â™«â™«â™« > ...waiting for response from OpenGrid Network... \n",
      "â™«â™«â™« > \u001b[92mDONE!\u001b[0m\n",
      "\n",
      "â™«â™«â™« > \u001b[92mCONNECTED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "duet1 = sy.join_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8482fa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤  ðŸŽ¸  â™ªâ™ªâ™ª Joining Duet â™«â™«â™«  ðŸŽ»  ðŸŽ¹\n",
      "\n",
      "â™«â™«â™« >\u001b[93m DISCLAIMER\u001b[0m: \u001b[1mDuet is an experimental feature currently in beta.\n",
      "â™«â™«â™« > Use at your own risk.\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "    > â¤ï¸ \u001b[91mLove\u001b[0m \u001b[92mDuet\u001b[0m? \u001b[93mPlease\u001b[0m \u001b[94mconsider\u001b[0m \u001b[95msupporting\u001b[0m \u001b[91mour\u001b[0m \u001b[93mcommunity!\u001b[0m\n",
      "    > https://github.com/sponsors/OpenMined\u001b[1m\n",
      "\n",
      "â™«â™«â™« > Punching through firewall to OpenGrid Network Node at:\n",
      "â™«â™«â™« > http://ec2-18-218-7-180.us-east-2.compute.amazonaws.com:5000\n",
      "â™«â™«â™« >\n",
      "â™«â™«â™« > ...waiting for response from OpenGrid Network... \n",
      "â™«â™«â™« > \u001b[92mDONE!\u001b[0m\n",
      "\n",
      "â™«â™«â™« > \u001b[95mSTEP 1:\u001b[0m Send the following Duet Client ID to your duet partner!\n",
      "â™«â™«â™« > Duet Client ID: \u001b[1m7b065fe7f5cbdd5e7a3a0221c36b5b35\u001b[0m\n",
      "\n",
      "â™«â™«â™« > ...waiting for partner to connect...\n",
      "\n",
      "â™«â™«â™« > \u001b[92mCONNECTED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "duet1 = sy.join_duet(\"f246064f24d360c68e7b410770dcc17d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce78bb8",
   "metadata": {},
   "source": [
    "## Join the Duet Server the Data Owner 2 connected to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bf2f929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤  ðŸŽ¸  â™ªâ™ªâ™ª Joining Duet â™«â™«â™«  ðŸŽ»  ðŸŽ¹\n",
      "\n",
      "â™«â™«â™« >\u001b[93m DISCLAIMER\u001b[0m: \u001b[1mDuet is an experimental feature currently in beta.\n",
      "â™«â™«â™« > Use at your own risk.\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "    > â¤ï¸ \u001b[91mLove\u001b[0m \u001b[92mDuet\u001b[0m? \u001b[93mPlease\u001b[0m \u001b[94mconsider\u001b[0m \u001b[95msupporting\u001b[0m \u001b[91mour\u001b[0m \u001b[93mcommunity!\u001b[0m\n",
      "    > https://github.com/sponsors/OpenMined\u001b[1m\n",
      "\n",
      "â™«â™«â™« > Punching through firewall to OpenGrid Network Node at:\n",
      "â™«â™«â™« > http://ec2-18-218-7-180.us-east-2.compute.amazonaws.com:5000\n",
      "â™«â™«â™« >\n",
      "â™«â™«â™« > ...waiting for response from OpenGrid Network... \n",
      "â™«â™«â™« > \u001b[92mDONE!\u001b[0m\n",
      "\n",
      "â™«â™«â™« > \u001b[92mCONNECTED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "duet2 = sy.join_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e2878",
   "metadata": {},
   "source": [
    "## Get data from Duet Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c55a5eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Description</th>\n",
       "      <th>object_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;UID: 525378a8d9624a9e886c8b2f3068baac&gt;</td>\n",
       "      <td>[X_Train_Owner1]</td>\n",
       "      <td>Dataset of 400 samples, 7 feature</td>\n",
       "      <td>&lt;class 'torch.Tensor'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;UID: 854a07c3bf394e44b5625c16364f9755&gt;</td>\n",
       "      <td>[y_Train_Owner1]</td>\n",
       "      <td>Dataset of 400 samples, 1 feature</td>\n",
       "      <td>&lt;class 'torch.Tensor'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;UID: f265f31823664e1788e931aadd2b2dbc&gt;</td>\n",
       "      <td>[X_Test_Owner1]</td>\n",
       "      <td>Dataset of 400 samples, 7 feature</td>\n",
       "      <td>&lt;class 'torch.Tensor'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;UID: 58deaf7d9f6140f084971563bd5da3e2&gt;</td>\n",
       "      <td>[y_Test_Owner1]</td>\n",
       "      <td>Dataset of 400 samples, 1 feature</td>\n",
       "      <td>&lt;class 'torch.Tensor'&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ID              Tags  \\\n",
       "0  <UID: 525378a8d9624a9e886c8b2f3068baac>  [X_Train_Owner1]   \n",
       "1  <UID: 854a07c3bf394e44b5625c16364f9755>  [y_Train_Owner1]   \n",
       "2  <UID: f265f31823664e1788e931aadd2b2dbc>   [X_Test_Owner1]   \n",
       "3  <UID: 58deaf7d9f6140f084971563bd5da3e2>   [y_Test_Owner1]   \n",
       "\n",
       "                         Description             object_type  \n",
       "0  Dataset of 400 samples, 7 feature  <class 'torch.Tensor'>  \n",
       "1  Dataset of 400 samples, 1 feature  <class 'torch.Tensor'>  \n",
       "2  Dataset of 400 samples, 7 feature  <class 'torch.Tensor'>  \n",
       "3  Dataset of 400 samples, 1 feature  <class 'torch.Tensor'>  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duet1.store.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7569d9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X_Train_Owner1']\n",
      "Dataset of 400 samples, 7 feature\n"
     ]
    }
   ],
   "source": [
    "X_Train_Owner1_ptr = duet1.store[0]\n",
    "y_Train_Owner1_ptr = duet1.store[1]\n",
    "X_Test_Owner1_ptr = duet1.store[2]\n",
    "y_Test_Owner1_ptr = duet1.store[3]\n",
    "\n",
    "print(X_Train_Owner1_ptr.tags)\n",
    "print(X_Train_Owner1_ptr.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81e8f725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Description</th>\n",
       "      <th>object_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;UID: 6dc8858a926a4745924b767ce88e7273&gt;</td>\n",
       "      <td>[X_Train_Owner2]</td>\n",
       "      <td>Dataset of 400 samples, 7 feature</td>\n",
       "      <td>&lt;class 'torch.Tensor'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;UID: 4184124411db439bb47960409c2bb9ef&gt;</td>\n",
       "      <td>[y_Train_Owner2]</td>\n",
       "      <td>Dataset of 400 samples, 1 feature</td>\n",
       "      <td>&lt;class 'torch.Tensor'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;UID: 78f059281d694172973cb23a78e5168e&gt;</td>\n",
       "      <td>[X_Test_Owner2]</td>\n",
       "      <td>Dataset of 400 samples, 7 feature</td>\n",
       "      <td>&lt;class 'torch.Tensor'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;UID: 21b40ddd1a614b58bc5ad245f7d20005&gt;</td>\n",
       "      <td>[y_Test_Owner2]</td>\n",
       "      <td>Dataset of 400 samples, 1 feature</td>\n",
       "      <td>&lt;class 'torch.Tensor'&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ID              Tags  \\\n",
       "0  <UID: 6dc8858a926a4745924b767ce88e7273>  [X_Train_Owner2]   \n",
       "1  <UID: 4184124411db439bb47960409c2bb9ef>  [y_Train_Owner2]   \n",
       "2  <UID: 78f059281d694172973cb23a78e5168e>   [X_Test_Owner2]   \n",
       "3  <UID: 21b40ddd1a614b58bc5ad245f7d20005>   [y_Test_Owner2]   \n",
       "\n",
       "                         Description             object_type  \n",
       "0  Dataset of 400 samples, 7 feature  <class 'torch.Tensor'>  \n",
       "1  Dataset of 400 samples, 1 feature  <class 'torch.Tensor'>  \n",
       "2  Dataset of 400 samples, 7 feature  <class 'torch.Tensor'>  \n",
       "3  Dataset of 400 samples, 1 feature  <class 'torch.Tensor'>  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duet2.store.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c0f5d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X_Train_Owner2']\n",
      "Dataset of 400 samples, 7 feature\n"
     ]
    }
   ],
   "source": [
    "X_Train_Owner2_ptr = duet2.store[0]\n",
    "y_Train_Owner2_ptr = duet2.store[1]\n",
    "X_Test_Owner2_ptr = duet2.store[2]\n",
    "y_Test_Owner2_ptr = duet2.store[3]\n",
    "\n",
    "print(X_Train_Owner2_ptr.tags)\n",
    "print(X_Train_Owner2_ptr.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd01d89",
   "metadata": {},
   "source": [
    "## Build a Base Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40a44ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyNet(sy.Module):\n",
    "    def __init__(self,input_channels,output_channels,torch_ref):\n",
    "        super(SyNet,self).__init__(torch_ref=torch_ref)\n",
    "        #Our network:\n",
    "        # Linear1->relu->Batchnorm->Linear2->relu->Batchnorm->Dropout->Linear3->output\n",
    "        # Softmax is added in the predict function\n",
    "        #This applies Linear transformation to input data. \n",
    "        self.fc1 = self.torch_ref.nn.Linear(input_channels,int(1.5*input_channels))\n",
    "        self.fc2 = self.torch_ref.nn.Linear(int(1.5*input_channels),int(1.5*input_channels))\n",
    "        self.fc3 = self.torch_ref.nn.Linear(int(1.5*input_channels),output_channels)\n",
    "        \n",
    "        self.relu = self.torch_ref.nn.ReLU()\n",
    "        self.dropout = self.torch_ref.nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = self.torch_ref.nn.BatchNorm1d(int(1.5*input_channels))\n",
    "        self.batchnorm2 = self.torch_ref.nn.BatchNorm1d(int(1.5*input_channels))\n",
    "        self.sigmoid = self.torch_ref.nn.Sigmoid()\n",
    "    #This must be implemented\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    def predict(self,x):\n",
    "        output = self.forward(x)\n",
    "        prediction = self.torch_ref.argmax(output,1)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "086f6a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_train(epochs, model, optimizer, X_ptr,y_ptr,criterion,torch_ref):\n",
    "    losses = []\n",
    "    for i in range(epochs):\n",
    "        #Precit the output for Given input\n",
    "        y_pred_ptr = model.forward(X_ptr)\n",
    "        #Compute Cross entropy loss\n",
    "        loss = criterion(y_pred_ptr,y_ptr)\n",
    "        loss_item = loss.item()\n",
    "        #Request to get the loss value\n",
    "        loss_value = loss_item.get(\n",
    "            reason=\"To evaluate training progress\",\n",
    "            request_block=True,\n",
    "            timeout_secs=5,\n",
    "        )\n",
    "        #Add loss to the list\n",
    "        losses.append(loss_value)\n",
    "        #Print loss\n",
    "        if i%50==0:\n",
    "            print(\"Epoch:\",i,\" Loss:\",loss_value)\n",
    "        \n",
    "        #Clear the previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        #Compute gradients\n",
    "        loss.backward()\n",
    "        #Adjust weights\n",
    "        optimizer.step()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4539d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as base_torch         \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14ecce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Classifier Model\n",
    "X_Train_Owner1_shape = X_Train_Owner1_ptr.shape.get(\n",
    "            reason=\"To evaluate training progress\",\n",
    "            request_block=True,\n",
    "            timeout_secs=5,\n",
    "        )\n",
    "input_channels = X_Train_Owner1_shape[1]\n",
    "output_channels = 2\n",
    "base_model = SyNet(input_channels,output_channels,base_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cc9031",
   "metadata": {},
   "source": [
    "## Send copy of model to Data Owner1and train them remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca5409a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 0.7004346251487732\n",
      "Epoch: 50  Loss: 0.6451839208602905\n",
      "Epoch: 100  Loss: 0.6058306694030762\n",
      "Epoch: 150  Loss: 0.5695232152938843\n",
      "Epoch: 200  Loss: 0.5338520407676697\n",
      "Epoch: 250  Loss: 0.5037806034088135\n",
      "Epoch: 300  Loss: 0.48326438665390015\n",
      "Epoch: 350  Loss: 0.4705788791179657\n",
      "Epoch: 400  Loss: 0.4617140591144562\n",
      "Epoch: 450  Loss: 0.44991788268089294\n",
      "Epoch: 500  Loss: 0.4418407082557678\n",
      "Epoch: 550  Loss: 0.44389012455940247\n",
      "Epoch: 600  Loss: 0.4369836449623108\n",
      "Epoch: 650  Loss: 0.4358045160770416\n",
      "Epoch: 700  Loss: 0.43021854758262634\n",
      "Epoch: 750  Loss: 0.4276730716228485\n",
      "Epoch: 800  Loss: 0.4326799809932709\n",
      "Epoch: 850  Loss: 0.4234877824783325\n",
      "Epoch: 900  Loss: 0.42232897877693176\n",
      "Epoch: 950  Loss: 0.41833141446113586\n"
     ]
    }
   ],
   "source": [
    "remote_model1 = base_model.send(duet1)\n",
    "\n",
    "remote_torch1 = duet1.torch\n",
    "params = remote_model1.parameters()\n",
    "#Define loss criterion\n",
    "criterion = remote_torch1.nn.CrossEntropyLoss()\n",
    "#Define the optimizer\n",
    "optimizer = remote_torch1.optim.Adam(params=params, lr=0.001)\n",
    "#Number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Training\n",
    "losses = classifier_train(epochs, remote_model1, optimizer, X_Train_Owner1_ptr, y_Train_Owner1_ptr,criterion, remote_torch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5e04b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_Test_Owner1_predict_ptr = remote_model1.predict(X_Test_Owner1_ptr)\n",
    "#s = y_Test_Owner1_predict_ptr-y_Test_Owner1_ptr\n",
    "#res = s.get(reason=\"To evaluate training progress\",\n",
    "#             request_block=True,\n",
    "#             timeout_secs=5,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa7d6e5",
   "metadata": {},
   "source": [
    "## Send copy of model to Data Owner2and train them remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0af8c9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 0.6966527700424194\n",
      "Epoch: 50  Loss: 0.6478937268257141\n",
      "Epoch: 100  Loss: 0.609880268573761\n",
      "Epoch: 150  Loss: 0.577053964138031\n",
      "Epoch: 200  Loss: 0.5489195585250854\n",
      "Epoch: 250  Loss: 0.5302548408508301\n",
      "Epoch: 300  Loss: 0.5106964707374573\n",
      "Epoch: 350  Loss: 0.5011875033378601\n",
      "Epoch: 400  Loss: 0.4858357310295105\n",
      "Epoch: 450  Loss: 0.4717283546924591\n",
      "Epoch: 500  Loss: 0.47332873940467834\n",
      "Epoch: 550  Loss: 0.470691978931427\n",
      "Epoch: 600  Loss: 0.4606500267982483\n",
      "Epoch: 650  Loss: 0.45962849259376526\n",
      "Epoch: 700  Loss: 0.45874762535095215\n",
      "Epoch: 750  Loss: 0.4518145024776459\n",
      "Epoch: 800  Loss: 0.4484209716320038\n",
      "Epoch: 850  Loss: 0.4407496750354767\n",
      "Epoch: 900  Loss: 0.4460342526435852\n",
      "Epoch: 950  Loss: 0.43799644708633423\n"
     ]
    }
   ],
   "source": [
    "remote_model2 = base_model.send(duet2)\n",
    "\n",
    "remote_torch2 = duet2.torch\n",
    "params = remote_model2.parameters()\n",
    "#Define loss criterion\n",
    "criterion = remote_torch2.nn.CrossEntropyLoss()\n",
    "#Define the optimizer\n",
    "optimizer = remote_torch2.optim.Adam(params=params, lr=0.001)\n",
    "#Number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Training\n",
    "losses = classifier_train(epochs, remote_model2, optimizer, X_Train_Owner2_ptr, y_Train_Owner2_ptr,criterion, remote_torch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b257d3d",
   "metadata": {},
   "source": [
    "## Averaging Model Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd7debc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d95faab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model parameters:\n",
      "[Parameter containing:\n",
      "tensor([[ 0.1314, -0.0100, -0.1370, -0.1415, -0.1813, -0.1167, -0.3393],\n",
      "        [ 0.0680, -0.2117,  0.1996,  0.1533,  0.2154,  0.1933, -0.0834],\n",
      "        [-0.2420,  0.0155,  0.1908, -0.2892,  0.0289, -0.2381, -0.1863],\n",
      "        [-0.2485, -0.1510,  0.2578,  0.1517,  0.1352, -0.0104, -0.0066],\n",
      "        [-0.1201, -0.0559, -0.1008,  0.2836, -0.1717, -0.2393, -0.2102],\n",
      "        [-0.1682, -0.0529, -0.2872, -0.0122, -0.1079,  0.0922,  0.0068],\n",
      "        [-0.2889,  0.0480, -0.2706, -0.2120, -0.0601, -0.3508,  0.2083],\n",
      "        [-0.1316, -0.3094, -0.3572, -0.0752, -0.3105, -0.2696, -0.1861],\n",
      "        [-0.3545,  0.3605, -0.0287, -0.3060,  0.3383,  0.0435,  0.2597],\n",
      "        [ 0.0729, -0.0745,  0.1798, -0.3712, -0.1393,  0.0786,  0.2052]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0526,  0.0100, -0.2898, -0.1563, -0.0824,  0.3321,  0.1174, -0.3229,\n",
      "        -0.0069, -0.0471], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0279, -0.0926,  0.2341, -0.2123, -0.0565, -0.0550,  0.2776,  0.1576,\n",
      "          0.1613,  0.0567],\n",
      "        [ 0.0741,  0.2082, -0.1207, -0.2582, -0.0914, -0.0600,  0.3001, -0.2197,\n",
      "          0.0791,  0.1631],\n",
      "        [-0.1919,  0.0200,  0.1351,  0.0922,  0.2188,  0.2552,  0.2582, -0.0242,\n",
      "         -0.0668,  0.0606],\n",
      "        [ 0.0935, -0.3006, -0.2926,  0.1044, -0.1883, -0.1821,  0.1731,  0.1493,\n",
      "          0.0826, -0.2592],\n",
      "        [-0.2608,  0.2510, -0.2993, -0.2788,  0.2699, -0.2336, -0.1226, -0.0263,\n",
      "          0.2839, -0.0825],\n",
      "        [-0.2846,  0.0788, -0.0232, -0.0981, -0.1090,  0.1590,  0.0302,  0.0068,\n",
      "         -0.1364,  0.1486],\n",
      "        [ 0.1713,  0.2127, -0.2165,  0.1941,  0.0026,  0.1980,  0.2974,  0.0926,\n",
      "          0.1746, -0.2801],\n",
      "        [ 0.3067, -0.1034,  0.0739, -0.0699, -0.1084,  0.2399,  0.1450, -0.2988,\n",
      "         -0.2894, -0.2133],\n",
      "        [-0.1124,  0.2539,  0.0729,  0.2488,  0.2545,  0.0912,  0.0802, -0.0048,\n",
      "         -0.1436, -0.1853],\n",
      "        [-0.1338, -0.2087,  0.1186,  0.2444, -0.1929, -0.2771,  0.0992,  0.0717,\n",
      "         -0.2205, -0.2694]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0823,  0.1070, -0.0258,  0.0141, -0.2304,  0.2498,  0.1786,  0.2132,\n",
      "        -0.0042,  0.1027], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.2178,  0.0927,  0.2614,  0.1568,  0.0730, -0.0494, -0.0314,  0.2983,\n",
      "          0.0622, -0.1678],\n",
      "        [ 0.0468, -0.0459,  0.1068, -0.1470, -0.0321,  0.1327, -0.2230,  0.3125,\n",
      "         -0.1784, -0.1369]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0103, -0.2370], requires_grad=True), Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)]\n",
      "\n",
      "Remote model1 parameters:\n",
      "[Parameter containing:\n",
      "tensor([[-9.3534e-03, -7.9372e-02, -2.2492e-01, -2.4150e-01, -2.7234e-01,\n",
      "         -9.3426e-02, -1.7859e-01],\n",
      "        [ 2.7532e-01, -6.0084e-02,  2.8861e-01,  6.4675e-02,  1.7945e-01,\n",
      "          4.7683e-04,  4.5436e-02],\n",
      "        [-8.1272e-02, -1.6110e-01,  1.3565e-01, -4.6696e-01,  1.5082e-01,\n",
      "         -2.8951e-01,  1.7403e-02],\n",
      "        [-1.2627e-02, -2.2554e-01,  2.2431e-01,  1.4033e-01,  6.1436e-02,\n",
      "          2.0859e-01,  7.6308e-02],\n",
      "        [-5.1726e-02, -9.9242e-03, -7.5698e-02,  1.2318e-01, -2.2598e-01,\n",
      "         -3.3482e-01, -2.4864e-01],\n",
      "        [-1.1376e-01,  3.1022e-02, -1.1888e-01, -3.0595e-02, -3.1798e-01,\n",
      "          2.2387e-01, -5.1228e-02],\n",
      "        [-3.2334e-01, -2.5574e-03, -9.0825e-02, -4.5668e-03, -4.2203e-01,\n",
      "         -2.7367e-01, -7.8237e-02],\n",
      "        [-3.5973e-01, -1.2180e-02, -3.2763e-01, -2.0225e-01, -4.4690e-01,\n",
      "         -2.7135e-01, -2.9107e-01],\n",
      "        [-3.7991e-01,  1.4592e-01,  1.1175e-01, -1.4534e-01,  5.2560e-01,\n",
      "         -2.6800e-02,  1.5147e-01],\n",
      "        [ 2.1716e-01, -1.1134e-01, -6.1877e-02, -3.0634e-01,  1.1144e-02,\n",
      "          9.9894e-02,  1.6993e-01]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.1298,  0.2011, -0.2590, -0.2063,  0.0696,  0.1562, -0.0411, -0.1279,\n",
      "        -0.1149, -0.2301], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0737,  0.1156, -0.0239, -0.2602, -0.3395, -0.1534,  0.2423,  0.1065,\n",
      "          0.1199, -0.0295],\n",
      "        [ 0.1594,  0.1910, -0.0448, -0.4022, -0.2806,  0.0468,  0.2650, -0.0493,\n",
      "          0.1803, -0.0698],\n",
      "        [ 0.2136,  0.1682,  0.0665,  0.0645, -0.0593,  0.1968,  0.3225, -0.2988,\n",
      "          0.0218, -0.1128],\n",
      "        [ 0.1898, -0.1962, -0.4135,  0.1131, -0.3469, -0.0719,  0.1800, -0.0120,\n",
      "          0.0557, -0.2451],\n",
      "        [-0.4556,  0.1731, -0.3304, -0.4100,  0.1091,  0.0912, -0.0177,  0.0100,\n",
      "          0.1498, -0.4170],\n",
      "        [-0.2244, -0.1066,  0.0495, -0.0818, -0.0543,  0.0044, -0.1188,  0.1956,\n",
      "         -0.1408,  0.0641],\n",
      "        [ 0.2420,  0.1501, -0.2168, -0.0090, -0.2048,  0.1216,  0.3153, -0.3117,\n",
      "         -0.0215, -0.1767],\n",
      "        [ 0.0398, -0.3924,  0.1743, -0.3932,  0.0310, -0.1952, -0.1421, -0.0412,\n",
      "         -0.3485,  0.0840],\n",
      "        [-0.1635,  0.1942,  0.0983,  0.0125,  0.2276,  0.1960,  0.1184, -0.2203,\n",
      "          0.0223, -0.0708],\n",
      "        [ 0.0901, -0.1413, -0.0546,  0.0411, -0.0866, -0.4637, -0.0755,  0.1566,\n",
      "         -0.1874, -0.4233]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.1815, -0.1130, -0.1759, -0.2694, -0.4224, -0.0734, -0.0555,  0.0716,\n",
      "        -0.3588, -0.0167], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.6846,  0.6442,  0.8329,  0.8740,  0.6228, -0.7502,  0.6352, -0.2059,\n",
      "          0.5644, -0.6424],\n",
      "        [-0.6349, -0.6723, -0.6140, -0.8018, -0.5828,  0.7417, -0.7167,  0.5343,\n",
      "         -0.6553,  0.5105]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.2160,  0.0340], requires_grad=True), Parameter containing:\n",
      "tensor([0.9963, 0.9287, 0.9241, 1.0561, 1.0085, 0.8563, 0.9664, 1.1225, 0.8810,\n",
      "        0.9031], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1950, -0.1405,  0.2284,  0.1524,  0.2127,  0.0167, -0.2275, -0.2298,\n",
      "        -0.2500,  0.2185], requires_grad=True), Parameter containing:\n",
      "tensor([1.8966, 1.8301, 1.9862, 1.9406, 1.8439, 1.9552, 1.7921, 1.0922, 1.6627,\n",
      "        1.8892], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4058, -0.3909, -0.3986, -0.3361, -0.3904,  0.3976, -0.3704,  0.5053,\n",
      "        -0.3393,  0.4295], requires_grad=True)]\n",
      "\n",
      "Remote model2 parameters:\n",
      "[Parameter containing:\n",
      "tensor([[-0.0385, -0.0058, -0.0874, -0.0627, -0.0527, -0.2703, -0.2677],\n",
      "        [ 0.0842, -0.0141,  0.1446,  0.1205,  0.1763,  0.4094, -0.0008],\n",
      "        [-0.2058,  0.1224,  0.2488, -0.2108,  0.0270, -0.1298, -0.1345],\n",
      "        [-0.2410, -0.0684,  0.1107,  0.0843,  0.1936,  0.1165,  0.1196],\n",
      "        [-0.0115,  0.0907, -0.1885,  0.3961, -0.0893, -0.0133, -0.2737],\n",
      "        [-0.1307,  0.0463, -0.0604, -0.2121, -0.1175,  0.0965,  0.1100],\n",
      "        [-0.4387, -0.1302, -0.2141, -0.2922, -0.0507, -0.0635,  0.2420],\n",
      "        [-0.3554, -0.2930, -0.0789, -0.2702, -0.0058, -0.2380, -0.0604],\n",
      "        [ 0.0489,  0.1013,  0.1856, -0.4216,  0.2920,  0.2903,  0.3645],\n",
      "        [-0.0147, -0.0349,  0.1769, -0.3142, -0.1322,  0.0656,  0.3571]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.3102,  0.0090, -0.4064, -0.2456, -0.1828,  0.3810, -0.2227, -0.5380,\n",
      "         0.2286, -0.0601], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.1252,  0.0289, -0.0100,  0.1800, -0.2801,  0.4099, -0.0518,  0.0515,\n",
      "         -0.1583,  0.0014],\n",
      "        [ 0.0868,  0.2149, -0.3863,  0.0242,  0.1036,  0.1508, -0.1021,  0.0650,\n",
      "         -0.1442,  0.2286],\n",
      "        [-0.0733, -0.2903,  0.0419,  0.2766,  0.1581,  0.2233,  0.0072,  0.0892,\n",
      "         -0.0778, -0.0873],\n",
      "        [-0.0639, -0.2877, -0.3125,  0.0845, -0.0573,  0.0060, -0.0822,  0.0422,\n",
      "          0.2102, -0.3443],\n",
      "        [-0.5325,  0.0290, -0.6018, -0.3712,  0.1106,  0.0774, -0.1969,  0.1418,\n",
      "          0.1002, -0.1639],\n",
      "        [-0.3144,  0.1575,  0.0202, -0.0459, -0.3014, -0.0162,  0.0581, -0.0802,\n",
      "         -0.1799,  0.0275],\n",
      "        [-0.1028,  0.1128, -0.3197,  0.2133,  0.2005,  0.4727, -0.1402,  0.2194,\n",
      "         -0.1693, -0.3460],\n",
      "        [ 0.2685,  0.0279, -0.0298,  0.0122, -0.2464, -0.0219,  0.3003, -0.4523,\n",
      "         -0.2516, -0.1393],\n",
      "        [-0.4655, -0.0387,  0.0237,  0.0729,  0.1207,  0.1625, -0.3075,  0.1363,\n",
      "         -0.0456, -0.0142],\n",
      "        [ 0.1704,  0.0759,  0.0397, -0.0682, -0.1738, -0.1917,  0.2314, -0.3023,\n",
      "         -0.2963,  0.1259]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.2778, -0.3215, -0.3742, -0.3361, -0.3984, -0.0792,  0.0706,  0.1592,\n",
      "        -0.1958,  0.0170], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.7951,  0.6963,  0.8484,  0.8221,  0.8475, -0.6740,  0.4981, -0.2209,\n",
      "          0.6383, -0.6123],\n",
      "        [-0.6807, -0.6794, -0.6565, -0.7981, -0.7845,  0.6728, -0.6249,  0.6362,\n",
      "         -0.6885,  0.2709]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.1925,  0.0337], requires_grad=True), Parameter containing:\n",
      "tensor([1.0890, 0.8984, 1.1714, 1.0525, 0.9355, 0.8655, 0.9255, 1.0518, 1.0184,\n",
      "        0.9926], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2802, -0.1000,  0.2156,  0.0358,  0.0341, -0.2965, -0.3507, -0.2665,\n",
      "        -0.0451, -0.0429], requires_grad=True), Parameter containing:\n",
      "tensor([1.9363, 1.8617, 1.9834, 1.8652, 2.0579, 1.7791, 1.5939, 1.3448, 1.7802,\n",
      "        1.6766], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3408, -0.3961, -0.3152, -0.2983, -0.3706,  0.3683, -0.3595,  0.4766,\n",
      "        -0.3275,  0.3958], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "param1 = remote_model1.parameters().get(request_block=True)\n",
    "param2 = remote_model2.parameters().get(request_block=True)\n",
    "\n",
    "print(\"Base Model parameters:\")\n",
    "print(base_model.parameters())\n",
    "print()\n",
    "\n",
    "print(\"Remote model1 parameters:\")\n",
    "print(param1)\n",
    "print()\n",
    "\n",
    "print(\"Remote model2 parameters:\")\n",
    "print(param2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd6b64dd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('fc1.weight', tensor([[-9.3534e-03, -7.9372e-02, -2.2492e-01, -2.4150e-01, -2.7234e-01,\n",
      "         -9.3426e-02, -1.7859e-01],\n",
      "        [ 2.7532e-01, -6.0084e-02,  2.8861e-01,  6.4675e-02,  1.7945e-01,\n",
      "          4.7683e-04,  4.5436e-02],\n",
      "        [-8.1272e-02, -1.6110e-01,  1.3565e-01, -4.6696e-01,  1.5082e-01,\n",
      "         -2.8951e-01,  1.7403e-02],\n",
      "        [-1.2627e-02, -2.2554e-01,  2.2431e-01,  1.4033e-01,  6.1436e-02,\n",
      "          2.0859e-01,  7.6308e-02],\n",
      "        [-5.1726e-02, -9.9242e-03, -7.5698e-02,  1.2318e-01, -2.2598e-01,\n",
      "         -3.3482e-01, -2.4864e-01],\n",
      "        [-1.1376e-01,  3.1022e-02, -1.1888e-01, -3.0595e-02, -3.1798e-01,\n",
      "          2.2387e-01, -5.1228e-02],\n",
      "        [-3.2334e-01, -2.5574e-03, -9.0825e-02, -4.5668e-03, -4.2203e-01,\n",
      "         -2.7367e-01, -7.8237e-02],\n",
      "        [-3.5973e-01, -1.2180e-02, -3.2763e-01, -2.0225e-01, -4.4690e-01,\n",
      "         -2.7135e-01, -2.9107e-01],\n",
      "        [-3.7991e-01,  1.4592e-01,  1.1175e-01, -1.4534e-01,  5.2560e-01,\n",
      "         -2.6800e-02,  1.5147e-01],\n",
      "        [ 2.1716e-01, -1.1134e-01, -6.1877e-02, -3.0634e-01,  1.1144e-02,\n",
      "          9.9894e-02,  1.6993e-01]])), ('fc1.bias', tensor([-0.1298,  0.2011, -0.2590, -0.2063,  0.0696,  0.1562, -0.0411, -0.1279,\n",
      "        -0.1149, -0.2301])), ('fc2.weight', tensor([[ 0.0737,  0.1156, -0.0239, -0.2602, -0.3395, -0.1534,  0.2423,  0.1065,\n",
      "          0.1199, -0.0295],\n",
      "        [ 0.1594,  0.1910, -0.0448, -0.4022, -0.2806,  0.0468,  0.2650, -0.0493,\n",
      "          0.1803, -0.0698],\n",
      "        [ 0.2136,  0.1682,  0.0665,  0.0645, -0.0593,  0.1968,  0.3225, -0.2988,\n",
      "          0.0218, -0.1128],\n",
      "        [ 0.1898, -0.1962, -0.4135,  0.1131, -0.3469, -0.0719,  0.1800, -0.0120,\n",
      "          0.0557, -0.2451],\n",
      "        [-0.4556,  0.1731, -0.3304, -0.4100,  0.1091,  0.0912, -0.0177,  0.0100,\n",
      "          0.1498, -0.4170],\n",
      "        [-0.2244, -0.1066,  0.0495, -0.0818, -0.0543,  0.0044, -0.1188,  0.1956,\n",
      "         -0.1408,  0.0641],\n",
      "        [ 0.2420,  0.1501, -0.2168, -0.0090, -0.2048,  0.1216,  0.3153, -0.3117,\n",
      "         -0.0215, -0.1767],\n",
      "        [ 0.0398, -0.3924,  0.1743, -0.3932,  0.0310, -0.1952, -0.1421, -0.0412,\n",
      "         -0.3485,  0.0840],\n",
      "        [-0.1635,  0.1942,  0.0983,  0.0125,  0.2276,  0.1960,  0.1184, -0.2203,\n",
      "          0.0223, -0.0708],\n",
      "        [ 0.0901, -0.1413, -0.0546,  0.0411, -0.0866, -0.4637, -0.0755,  0.1566,\n",
      "         -0.1874, -0.4233]])), ('fc2.bias', tensor([-0.1815, -0.1130, -0.1759, -0.2694, -0.4224, -0.0734, -0.0555,  0.0716,\n",
      "        -0.3588, -0.0167])), ('fc3.weight', tensor([[ 0.6846,  0.6442,  0.8329,  0.8740,  0.6228, -0.7502,  0.6352, -0.2059,\n",
      "          0.5644, -0.6424],\n",
      "        [-0.6349, -0.6723, -0.6140, -0.8018, -0.5828,  0.7417, -0.7167,  0.5343,\n",
      "         -0.6553,  0.5105]])), ('fc3.bias', tensor([-0.2160,  0.0340])), ('batchnorm1.weight', tensor([0.9963, 0.9287, 0.9241, 1.0561, 1.0085, 0.8563, 0.9664, 1.1225, 0.8810,\n",
      "        0.9031])), ('batchnorm1.bias', tensor([ 0.1950, -0.1405,  0.2284,  0.1524,  0.2127,  0.0167, -0.2275, -0.2298,\n",
      "        -0.2500,  0.2185])), ('batchnorm1.running_mean', tensor([0.1227, 0.2793, 0.0980, 0.0953, 0.2714, 0.2207, 0.2505, 0.2583, 0.2115,\n",
      "        0.0913])), ('batchnorm1.running_var', tensor([0.0443, 0.1381, 0.0598, 0.0411, 0.0967, 0.0386, 0.0905, 0.1415, 0.1599,\n",
      "        0.0465])), ('batchnorm1.num_batches_tracked', tensor(1000)), ('batchnorm2.weight', tensor([1.8966, 1.8301, 1.9862, 1.9406, 1.8439, 1.9552, 1.7921, 1.0922, 1.6627,\n",
      "        1.8892])), ('batchnorm2.bias', tensor([-0.4058, -0.3909, -0.3986, -0.3361, -0.3904,  0.3976, -0.3704,  0.5053,\n",
      "        -0.3393,  0.4295])), ('batchnorm2.running_mean', tensor([0.0079, 0.0192, 0.0183, 0.0109, 0.0027, 0.0320, 0.0334, 0.4342, 0.0132,\n",
      "        0.1360])), ('batchnorm2.running_var', tensor([0.0012, 0.0040, 0.0032, 0.0017, 0.0002, 0.0027, 0.0045, 0.1087, 0.0024,\n",
      "        0.0265])), ('batchnorm2.num_batches_tracked', tensor(1000))])\n",
      "OrderedDict([('fc1.weight', tensor([[-0.0385, -0.0058, -0.0874, -0.0627, -0.0527, -0.2703, -0.2677],\n",
      "        [ 0.0842, -0.0141,  0.1446,  0.1205,  0.1763,  0.4094, -0.0008],\n",
      "        [-0.2058,  0.1224,  0.2488, -0.2108,  0.0270, -0.1298, -0.1345],\n",
      "        [-0.2410, -0.0684,  0.1107,  0.0843,  0.1936,  0.1165,  0.1196],\n",
      "        [-0.0115,  0.0907, -0.1885,  0.3961, -0.0893, -0.0133, -0.2737],\n",
      "        [-0.1307,  0.0463, -0.0604, -0.2121, -0.1175,  0.0965,  0.1100],\n",
      "        [-0.4387, -0.1302, -0.2141, -0.2922, -0.0507, -0.0635,  0.2420],\n",
      "        [-0.3554, -0.2930, -0.0789, -0.2702, -0.0058, -0.2380, -0.0604],\n",
      "        [ 0.0489,  0.1013,  0.1856, -0.4216,  0.2920,  0.2903,  0.3645],\n",
      "        [-0.0147, -0.0349,  0.1769, -0.3142, -0.1322,  0.0656,  0.3571]])), ('fc1.bias', tensor([-0.3102,  0.0090, -0.4064, -0.2456, -0.1828,  0.3810, -0.2227, -0.5380,\n",
      "         0.2286, -0.0601])), ('fc2.weight', tensor([[ 0.1252,  0.0289, -0.0100,  0.1800, -0.2801,  0.4099, -0.0518,  0.0515,\n",
      "         -0.1583,  0.0014],\n",
      "        [ 0.0868,  0.2149, -0.3863,  0.0242,  0.1036,  0.1508, -0.1021,  0.0650,\n",
      "         -0.1442,  0.2286],\n",
      "        [-0.0733, -0.2903,  0.0419,  0.2766,  0.1581,  0.2233,  0.0072,  0.0892,\n",
      "         -0.0778, -0.0873],\n",
      "        [-0.0639, -0.2877, -0.3125,  0.0845, -0.0573,  0.0060, -0.0822,  0.0422,\n",
      "          0.2102, -0.3443],\n",
      "        [-0.5325,  0.0290, -0.6018, -0.3712,  0.1106,  0.0774, -0.1969,  0.1418,\n",
      "          0.1002, -0.1639],\n",
      "        [-0.3144,  0.1575,  0.0202, -0.0459, -0.3014, -0.0162,  0.0581, -0.0802,\n",
      "         -0.1799,  0.0275],\n",
      "        [-0.1028,  0.1128, -0.3197,  0.2133,  0.2005,  0.4727, -0.1402,  0.2194,\n",
      "         -0.1693, -0.3460],\n",
      "        [ 0.2685,  0.0279, -0.0298,  0.0122, -0.2464, -0.0219,  0.3003, -0.4523,\n",
      "         -0.2516, -0.1393],\n",
      "        [-0.4655, -0.0387,  0.0237,  0.0729,  0.1207,  0.1625, -0.3075,  0.1363,\n",
      "         -0.0456, -0.0142],\n",
      "        [ 0.1704,  0.0759,  0.0397, -0.0682, -0.1738, -0.1917,  0.2314, -0.3023,\n",
      "         -0.2963,  0.1259]])), ('fc2.bias', tensor([-0.2778, -0.3215, -0.3742, -0.3361, -0.3984, -0.0792,  0.0706,  0.1592,\n",
      "        -0.1958,  0.0170])), ('fc3.weight', tensor([[ 0.7951,  0.6963,  0.8484,  0.8221,  0.8475, -0.6740,  0.4981, -0.2209,\n",
      "          0.6383, -0.6123],\n",
      "        [-0.6807, -0.6794, -0.6565, -0.7981, -0.7845,  0.6728, -0.6249,  0.6362,\n",
      "         -0.6885,  0.2709]])), ('fc3.bias', tensor([-0.1925,  0.0337])), ('batchnorm1.weight', tensor([1.0890, 0.8984, 1.1714, 1.0525, 0.9355, 0.8655, 0.9255, 1.0518, 1.0184,\n",
      "        0.9926])), ('batchnorm1.bias', tensor([ 0.2802, -0.1000,  0.2156,  0.0358,  0.0341, -0.2965, -0.3507, -0.2665,\n",
      "        -0.0451, -0.0429])), ('batchnorm1.running_mean', tensor([0.0463, 0.2327, 0.0246, 0.0715, 0.1390, 0.3992, 0.1460, 0.0497, 0.4614,\n",
      "        0.1893])), ('batchnorm1.running_var', tensor([0.0117, 0.1702, 0.0077, 0.0325, 0.0591, 0.0660, 0.0673, 0.0185, 0.4127,\n",
      "        0.0762])), ('batchnorm1.num_batches_tracked', tensor(1000)), ('batchnorm2.weight', tensor([1.9363, 1.8617, 1.9834, 1.8652, 2.0579, 1.7791, 1.5939, 1.3448, 1.7802,\n",
      "        1.6766])), ('batchnorm2.bias', tensor([-0.3408, -0.3961, -0.3152, -0.2983, -0.3706,  0.3683, -0.3595,  0.4766,\n",
      "        -0.3275,  0.3958])), ('batchnorm2.running_mean', tensor([0.0328, 0.0148, 0.0078, 0.0027, 0.0140, 0.0709, 0.1354, 0.3334, 0.0202,\n",
      "        0.1999])), ('batchnorm2.running_var', tensor([1.0459e-02, 3.9690e-03, 1.0650e-03, 1.1431e-04, 1.7765e-03, 8.4982e-03,\n",
      "        4.7637e-02, 1.3370e-01, 2.0208e-03, 5.0396e-02])), ('batchnorm2.num_batches_tracked', tensor(1000))])\n"
     ]
    }
   ],
   "source": [
    "remote_model1_updates = remote_model1.get(\n",
    "    request_block=True\n",
    ").state_dict()\n",
    "\n",
    "print(remote_model1_updates)\n",
    "\n",
    "remote_model2_updates = remote_model2.get(\n",
    "    request_block=True\n",
    ").state_dict()\n",
    "\n",
    "print(remote_model2_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95d18df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Average Model\n",
    "avg_updates = OrderedDict()\n",
    "for key in remote_model1_updates.keys():\n",
    "    avg_updates[key] = (remote_model1_updates[key] + remote_model2_updates[key]) / 2\n",
    "\n",
    "    \n",
    "combined_model = SyNet(input_channels,output_channels,base_torch)\n",
    "\n",
    "combined_model.load_state_dict(avg_updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5ac0b2",
   "metadata": {},
   "source": [
    "## Test the prediction of combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0ee305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "german = pd.read_csv(\"./data/german_data_2.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4666561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.20\n",
    "\n",
    "processed_data = None\n",
    "categorical = None\n",
    "label_encoders = {}\n",
    "\n",
    "def preprocessing(dataset, data, test_size):\n",
    "    \"\"\"\n",
    "    Preprocess dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: DataFrame\n",
    "        Pandas dataframe containing German dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    global processed_data\n",
    "    global categorical\n",
    "    global label_encoders\n",
    "\n",
    "    # Reset global variables\n",
    "    \n",
    "    processed_data = None\n",
    "    categorical = None\n",
    "    label_encoders = {}\n",
    "\n",
    "\n",
    "    if dataset == \"German\":\n",
    "        # Drop savings account and checkings account columns as they contain a lot\n",
    "        # of NaN values and may not always be available in real life scenarios\n",
    "        data = data.drop(columns = ['Saving accounts', 'Checking account'])\n",
    "        \n",
    "    dat_dict = data.to_dict()\n",
    "    new_dat_dict = {}\n",
    "\n",
    "    # rename columns(Make them lowercase and snakecase)\n",
    "    for key, value in dat_dict.items():\n",
    "        newKey = key\n",
    "        if type(key) == str:\n",
    "            newKey = newKey.lower().replace(' ', '_')\n",
    "        # if newKey != key:\n",
    "        new_dat_dict[newKey] = dat_dict[key]\n",
    "    del dat_dict\n",
    "\n",
    "    data = pd.DataFrame.from_dict(new_dat_dict)\n",
    "    del new_dat_dict\n",
    "\n",
    "\n",
    "    # print(data.describe())\n",
    "    # print(data.describe(include='O'))\n",
    "\n",
    "    cols = data.columns\n",
    "    num_cols = data._get_numeric_data().columns\n",
    "    categorical = list(set(cols) - set(num_cols))\n",
    "\n",
    "    # Drop null rows\n",
    "    data = data.dropna()\n",
    "\n",
    "    # Encode text columns to number values\n",
    "    for category in categorical:\n",
    "        le = LabelEncoder()\n",
    "        data[category] = le.fit_transform(data[category])\n",
    "        label_encoders[category] = le\n",
    "\n",
    "    for col in data.columns:\n",
    "        if(col not in categorical):\n",
    "            data[col] = (data[col].astype('float') - np.mean(data[col].astype('float')))/np.std(data[col].astype('float'))\n",
    "\n",
    "    # print(data.describe())\n",
    "    # print(data.describe(include='O'))\n",
    "\n",
    "    processed_data = data\n",
    "\n",
    "    # Get Training parameters\n",
    "    if dataset == \"German\":\n",
    "        target_col = data.columns[-1]\n",
    "        x = data.drop(columns=target_col, axis=1)\n",
    "        y = data[target_col].astype('int')\n",
    "    elif dataset == \"Australian\":\n",
    "        x = data.drop(14, axis=1)\n",
    "        y = data[14].astype('int')\n",
    "    elif dataset == \"Japanese\":\n",
    "        x = data.drop(15, axis=1)\n",
    "        y = data[15].astype('int')\n",
    "    elif dataset == \"Taiwan\":\n",
    "        x = data.drop('default_payment_next_month', axis=1)\n",
    "        y = data['default_payment_next_month'].astype('int')\n",
    "    elif dataset == \"Polish\":\n",
    "        x = data.drop('class', axis=1)\n",
    "        y = data['class'].astype('int')\n",
    "\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = test_size)\n",
    "    x_train = pd.DataFrame(x_train)\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    x_train = sc.fit_transform(x_train)\n",
    "    x_test = sc.transform(x_test)\n",
    "    \n",
    "    y_train = y_train[y_train.columns[0]].to_numpy()\n",
    "    y_test = y_test[y_test.columns[0]].to_numpy()\n",
    "\n",
    "    return (x_train, x_test, y_train, y_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocessing(\"German\", german, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1374a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trnasform the input to tensor\n",
    "X_test_tensor = base_torch.FloatTensor(X_test)\n",
    "y_test_tensor = base_torch.tensor(y_test,dtype=base_torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a7626dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcualte Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def accuracy(model,X,y):\n",
    "    print(accuracy_score(model.predict(X),y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c094a5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74\n"
     ]
    }
   ],
   "source": [
    "accuracy(combined_model,X_test_tensor,y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87666450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
