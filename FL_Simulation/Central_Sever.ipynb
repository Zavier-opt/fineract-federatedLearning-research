{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69ada23f",
   "metadata": {},
   "source": [
    "## Join the Duet Server the Data Owner 1 connected to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d6dfd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e78b315c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤  ðŸŽ¸  â™ªâ™ªâ™ª Joining Duet â™«â™«â™«  ðŸŽ»  ðŸŽ¹\n",
      "\n",
      "â™«â™«â™« >\u001b[93m DISCLAIMER\u001b[0m: \u001b[1mDuet is an experimental feature currently in beta.\n",
      "â™«â™«â™« > Use at your own risk.\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "    > â¤ï¸ \u001b[91mLove\u001b[0m \u001b[92mDuet\u001b[0m? \u001b[93mPlease\u001b[0m \u001b[94mconsider\u001b[0m \u001b[95msupporting\u001b[0m \u001b[91mour\u001b[0m \u001b[93mcommunity!\u001b[0m\n",
      "    > https://github.com/sponsors/OpenMined\u001b[1m\n",
      "\n",
      "â™«â™«â™« > Punching through firewall to OpenGrid Network Node at:\n",
      "â™«â™«â™« > http://ec2-18-218-7-180.us-east-2.compute.amazonaws.com:5000\n",
      "â™«â™«â™« >\n",
      "â™«â™«â™« > ...waiting for response from OpenGrid Network... \n",
      "â™«â™«â™« > \u001b[92mDONE!\u001b[0m\n",
      "\n",
      "â™«â™«â™« > \u001b[92mCONNECTED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "duet1 = sy.join_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8482fa6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bce78bb8",
   "metadata": {},
   "source": [
    "## Join the Duet Server the Data Owner 2 connected to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf2f929",
   "metadata": {},
   "outputs": [],
   "source": [
    "duet2 = sy.join_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e2878",
   "metadata": {},
   "source": [
    "## Get data from Duet Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7569d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train_Owner1_ptr = duet1.store[0]\n",
    "y_Train_Owner1_ptr = duet1.store[1]\n",
    "X_Test_Owner1_ptr = duet1.store[2]\n",
    "y_Test_Owner1_ptr = duet1.store[3]\n",
    "\n",
    "print(X_Train_Owner1_ptr.tags)\n",
    "print(X_Train_Owner1_ptr.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e8f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "duet2.store.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0f5d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train_Owner2_ptr = duet2.store[0]\n",
    "y_Train_Owner2_ptr = duet2.store[1]\n",
    "X_Test_Owner2_ptr = duet2.store[2]\n",
    "y_Test_Owner2_ptr = duet2.store[3]\n",
    "\n",
    "print(X_Train_Owner2_ptr.tags)\n",
    "print(X_Train_Owner2_ptr.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd01d89",
   "metadata": {},
   "source": [
    "## Build a Base Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a44ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyNet(sy.Module):\n",
    "    def __init__(self,input_channels,output_channels,torch_ref):\n",
    "        super(SyNet,self).__init__(torch_ref=torch_ref)\n",
    "        #Our network:\n",
    "        # Linear1->relu->Batchnorm->Linear2->relu->Batchnorm->Dropout->Linear3->output\n",
    "        # Softmax is added in the predict function\n",
    "        #This applies Linear transformation to input data. \n",
    "        self.fc1 = self.torch_ref.nn.Linear(input_channels,int(1.5*input_channels))\n",
    "        self.fc2 = self.torch_ref.nn.Linear(int(1.5*input_channels),int(1.5*input_channels))\n",
    "        self.fc3 = self.torch_ref.nn.Linear(int(1.5*input_channels),output_channels)\n",
    "        \n",
    "        self.relu = self.torch_ref.nn.ReLU()\n",
    "        self.dropout = self.torch_ref.nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = self.torch_ref.nn.BatchNorm1d(int(1.5*input_channels))\n",
    "        self.batchnorm2 = self.torch_ref.nn.BatchNorm1d(int(1.5*input_channels))\n",
    "        self.sigmoid = self.torch_ref.nn.Sigmoid()\n",
    "    #This must be implemented\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    def predict(self,x):\n",
    "        output = self.forward(x)\n",
    "        prediction = self.torch_ref.argmax(output,1)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086f6a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_train(epochs, model, optimizer, X_ptr,y_ptr,criterion,torch_ref):\n",
    "    losses = []\n",
    "    for i in range(epochs):\n",
    "        #Precit the output for Given input\n",
    "        y_pred_ptr = model.forward(X_ptr)\n",
    "        #Compute Cross entropy loss\n",
    "        loss = criterion(y_pred_ptr,y_ptr)\n",
    "        loss_item = loss.item()\n",
    "        #Request to get the loss value\n",
    "        loss_value = loss_item.get(\n",
    "            reason=\"To evaluate training progress\",\n",
    "            request_block=True,\n",
    "            timeout_secs=5,\n",
    "        )\n",
    "        #Add loss to the list\n",
    "        losses.append(loss_value)\n",
    "        #Print loss\n",
    "        if i%50==0:\n",
    "            print(\"Epoch:\",i,\" Loss:\",loss_value)\n",
    "        \n",
    "        #Clear the previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        #Compute gradients\n",
    "        loss.backward()\n",
    "        #Adjust weights\n",
    "        optimizer.step()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4539d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as base_torch         \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ecce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Classifier Model\n",
    "X_Train_Owner1_shape = X_Train_Owner1_ptr.shape.get(\n",
    "            reason=\"To evaluate training progress\",\n",
    "            request_block=True,\n",
    "            timeout_secs=5,\n",
    "        )\n",
    "input_channels = X_Train_Owner1_shape[1]\n",
    "output_channels = 2\n",
    "base_model = SyNet(input_channels,output_channels,base_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cc9031",
   "metadata": {},
   "source": [
    "## Send copy of model to Data Owner1and train them remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5409a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_model1 = base_model.send(duet1)\n",
    "\n",
    "remote_torch1 = duet1.torch\n",
    "params = remote_model1.parameters()\n",
    "#Define loss criterion\n",
    "criterion = remote_torch1.nn.CrossEntropyLoss()\n",
    "#Define the optimizer\n",
    "optimizer = remote_torch1.optim.Adam(params=params, lr=0.001)\n",
    "#Number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Training\n",
    "losses = classifier_train(epochs, remote_model1, optimizer, X_Train_Owner1_ptr, y_Train_Owner1_ptr,criterion, remote_torch1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa7d6e5",
   "metadata": {},
   "source": [
    "## Send copy of model to Data Owner2and train them remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af8c9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_model2 = base_model.send(duet2)\n",
    "\n",
    "remote_torch2 = duet2.torch\n",
    "params = remote_model2.parameters()\n",
    "#Define loss criterion\n",
    "criterion = remote_torch2.nn.CrossEntropyLoss()\n",
    "#Define the optimizer\n",
    "optimizer = remote_torch2.optim.Adam(params=params, lr=0.001)\n",
    "#Number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Training\n",
    "losses = classifier_train(epochs, remote_model2, optimizer, X_Train_Owner2_ptr, y_Train_Owner2_ptr,criterion, remote_torch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b257d3d",
   "metadata": {},
   "source": [
    "## Averaging Model Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7debc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95faab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "param1 = remote_model1.parameters().get(request_block=True)\n",
    "param2 = remote_model2.parameters().get(request_block=True)\n",
    "\n",
    "print(\"Base Model parameters:\")\n",
    "print(base_model.parameters())\n",
    "print()\n",
    "\n",
    "print(\"Remote model1 parameters:\")\n",
    "print(param1)\n",
    "print()\n",
    "\n",
    "print(\"Remote model2 parameters:\")\n",
    "print(param2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6b64dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_model1_updates = remote_model1.get(\n",
    "    request_block=True\n",
    ").state_dict()\n",
    "\n",
    "print(remote_model1_updates)\n",
    "\n",
    "remote_model2_updates = remote_model2.get(\n",
    "    request_block=True\n",
    ").state_dict()\n",
    "\n",
    "print(remote_model2_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d18df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Average Model\n",
    "avg_updates = OrderedDict()\n",
    "for key in remote_model1_updates.keys():\n",
    "    avg_updates[key] = (remote_model1_updates[key] + remote_model2_updates[key]) / 2\n",
    "\n",
    "    \n",
    "combined_model = SyNet(input_channels,output_channels,base_torch)\n",
    "\n",
    "combined_model.load_state_dict(avg_updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5ac0b2",
   "metadata": {},
   "source": [
    "## Test the prediction of combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ee305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "german = pd.read_csv(\"./data/german_data_2.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4666561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.20\n",
    "\n",
    "processed_data = None\n",
    "categorical = None\n",
    "label_encoders = {}\n",
    "\n",
    "def preprocessing(dataset, data, test_size):\n",
    "    \"\"\"\n",
    "    Preprocess dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: DataFrame\n",
    "        Pandas dataframe containing German dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    global processed_data\n",
    "    global categorical\n",
    "    global label_encoders\n",
    "\n",
    "    # Reset global variables\n",
    "    \n",
    "    processed_data = None\n",
    "    categorical = None\n",
    "    label_encoders = {}\n",
    "\n",
    "\n",
    "    if dataset == \"German\":\n",
    "        # Drop savings account and checkings account columns as they contain a lot\n",
    "        # of NaN values and may not always be available in real life scenarios\n",
    "        data = data.drop(columns = ['Saving accounts', 'Checking account'])\n",
    "        \n",
    "    dat_dict = data.to_dict()\n",
    "    new_dat_dict = {}\n",
    "\n",
    "    # rename columns(Make them lowercase and snakecase)\n",
    "    for key, value in dat_dict.items():\n",
    "        newKey = key\n",
    "        if type(key) == str:\n",
    "            newKey = newKey.lower().replace(' ', '_')\n",
    "        # if newKey != key:\n",
    "        new_dat_dict[newKey] = dat_dict[key]\n",
    "    del dat_dict\n",
    "\n",
    "    data = pd.DataFrame.from_dict(new_dat_dict)\n",
    "    del new_dat_dict\n",
    "\n",
    "\n",
    "    # print(data.describe())\n",
    "    # print(data.describe(include='O'))\n",
    "\n",
    "    cols = data.columns\n",
    "    num_cols = data._get_numeric_data().columns\n",
    "    categorical = list(set(cols) - set(num_cols))\n",
    "\n",
    "    # Drop null rows\n",
    "    data = data.dropna()\n",
    "\n",
    "    # Encode text columns to number values\n",
    "    for category in categorical:\n",
    "        le = LabelEncoder()\n",
    "        data[category] = le.fit_transform(data[category])\n",
    "        label_encoders[category] = le\n",
    "\n",
    "    for col in data.columns:\n",
    "        if(col not in categorical):\n",
    "            data[col] = (data[col].astype('float') - np.mean(data[col].astype('float')))/np.std(data[col].astype('float'))\n",
    "\n",
    "    # print(data.describe())\n",
    "    # print(data.describe(include='O'))\n",
    "\n",
    "    processed_data = data\n",
    "\n",
    "    # Get Training parameters\n",
    "    if dataset == \"German\":\n",
    "        target_col = data.columns[-1]\n",
    "        x = data.drop(columns=target_col, axis=1)\n",
    "        y = data[target_col].astype('int')\n",
    "    elif dataset == \"Australian\":\n",
    "        x = data.drop(14, axis=1)\n",
    "        y = data[14].astype('int')\n",
    "    elif dataset == \"Japanese\":\n",
    "        x = data.drop(15, axis=1)\n",
    "        y = data[15].astype('int')\n",
    "    elif dataset == \"Taiwan\":\n",
    "        x = data.drop('default_payment_next_month', axis=1)\n",
    "        y = data['default_payment_next_month'].astype('int')\n",
    "    elif dataset == \"Polish\":\n",
    "        x = data.drop('class', axis=1)\n",
    "        y = data['class'].astype('int')\n",
    "\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = test_size)\n",
    "    x_train = pd.DataFrame(x_train)\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    x_train = sc.fit_transform(x_train)\n",
    "    x_test = sc.transform(x_test)\n",
    "    \n",
    "    y_train = y_train[y_train.columns[0]].to_numpy()\n",
    "    y_test = y_test[y_test.columns[0]].to_numpy()\n",
    "\n",
    "    return (x_train, x_test, y_train, y_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocessing(\"German\", german, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1374a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trnasform the input to tensor\n",
    "X_test_tensor = base_torch.FloatTensor(X_test)\n",
    "y_test_tensor = base_torch.tensor(y_test,dtype=base_torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7626dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcualte Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def accuracy(model,X,y):\n",
    "    print(accuracy_score(model.predict(X),y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c094a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(combined_model,X_test_tensor,y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87666450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
