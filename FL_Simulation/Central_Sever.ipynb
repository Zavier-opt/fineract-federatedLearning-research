{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69ada23f",
   "metadata": {},
   "source": [
    "## Join the Duet Server the Data Owner 1 connected to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d6dfd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e78b315c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤  ðŸŽ¸  â™ªâ™ªâ™ª Joining Duet â™«â™«â™«  ðŸŽ»  ðŸŽ¹\n",
      "\n",
      "â™«â™«â™« >\u001b[93m DISCLAIMER\u001b[0m: \u001b[1mDuet is an experimental feature currently in beta.\n",
      "â™«â™«â™« > Use at your own risk.\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "    > â¤ï¸ \u001b[91mLove\u001b[0m \u001b[92mDuet\u001b[0m? \u001b[93mPlease\u001b[0m \u001b[94mconsider\u001b[0m \u001b[95msupporting\u001b[0m \u001b[91mour\u001b[0m \u001b[93mcommunity!\u001b[0m\n",
      "    > https://github.com/sponsors/OpenMined\u001b[1m\n",
      "\n",
      "â™«â™«â™« > Punching through firewall to OpenGrid Network Node at:\n",
      "â™«â™«â™« > http://ec2-18-218-7-180.us-east-2.compute.amazonaws.com:5000\n",
      "â™«â™«â™« >\n",
      "â™«â™«â™« > ...waiting for response from OpenGrid Network... \n",
      "â™«â™«â™« > \u001b[92mDONE!\u001b[0m\n",
      "\n",
      "â™«â™«â™« > \u001b[92mCONNECTED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "duet1 = sy.join_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce78bb8",
   "metadata": {},
   "source": [
    "## Join the Duet Server the Data Owner 2 connected to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8bf2f929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤  ðŸŽ¸  â™ªâ™ªâ™ª Joining Duet â™«â™«â™«  ðŸŽ»  ðŸŽ¹\n",
      "\n",
      "â™«â™«â™« >\u001b[93m DISCLAIMER\u001b[0m: \u001b[1mDuet is an experimental feature currently in beta.\n",
      "â™«â™«â™« > Use at your own risk.\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "    > â¤ï¸ \u001b[91mLove\u001b[0m \u001b[92mDuet\u001b[0m? \u001b[93mPlease\u001b[0m \u001b[94mconsider\u001b[0m \u001b[95msupporting\u001b[0m \u001b[91mour\u001b[0m \u001b[93mcommunity!\u001b[0m\n",
      "    > https://github.com/sponsors/OpenMined\u001b[1m\n",
      "\n",
      "â™«â™«â™« > Punching through firewall to OpenGrid Network Node at:\n",
      "â™«â™«â™« > http://ec2-18-218-7-180.us-east-2.compute.amazonaws.com:5000\n",
      "â™«â™«â™« >\n",
      "â™«â™«â™« > ...waiting for response from OpenGrid Network... \n",
      "â™«â™«â™« > \u001b[92mDONE!\u001b[0m\n",
      "\n",
      "â™«â™«â™« > \u001b[92mCONNECTED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "duet2 = sy.join_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e2878",
   "metadata": {},
   "source": [
    "## Get data from Duet Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c55a5eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Description</th>\n",
       "      <th>object_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;UID: 27034ee5bb12433b9a5ad4f8d292f7a2&gt;</td>\n",
       "      <td>[X_Train_Owner1]</td>\n",
       "      <td>Dataset of 400 samples, 7 feature</td>\n",
       "      <td>&lt;class 'torch.Tensor'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;UID: 6a775e86c0534b03a3acebdead61c60f&gt;</td>\n",
       "      <td>[y_Train_Owner1]</td>\n",
       "      <td>Dataset of 400 samples, 1 feature</td>\n",
       "      <td>&lt;class 'torch.Tensor'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;UID: 65072e4612e44795b753a24d66cd0d66&gt;</td>\n",
       "      <td>[X_Test_Owner1]</td>\n",
       "      <td>Dataset of 400 samples, 7 feature</td>\n",
       "      <td>&lt;class 'torch.Tensor'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;UID: 3f08d213eece4aaa8b5a2d281095c2fc&gt;</td>\n",
       "      <td>[y_Test_Owner1]</td>\n",
       "      <td>Dataset of 400 samples, 1 feature</td>\n",
       "      <td>&lt;class 'torch.Tensor'&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ID              Tags  \\\n",
       "0  <UID: 27034ee5bb12433b9a5ad4f8d292f7a2>  [X_Train_Owner1]   \n",
       "1  <UID: 6a775e86c0534b03a3acebdead61c60f>  [y_Train_Owner1]   \n",
       "2  <UID: 65072e4612e44795b753a24d66cd0d66>   [X_Test_Owner1]   \n",
       "3  <UID: 3f08d213eece4aaa8b5a2d281095c2fc>   [y_Test_Owner1]   \n",
       "\n",
       "                         Description             object_type  \n",
       "0  Dataset of 400 samples, 7 feature  <class 'torch.Tensor'>  \n",
       "1  Dataset of 400 samples, 1 feature  <class 'torch.Tensor'>  \n",
       "2  Dataset of 400 samples, 7 feature  <class 'torch.Tensor'>  \n",
       "3  Dataset of 400 samples, 1 feature  <class 'torch.Tensor'>  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duet1.store.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7569d9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X_Train_Owner1']\n",
      "Dataset of 400 samples, 7 feature\n"
     ]
    }
   ],
   "source": [
    "X_Train_Owner1_ptr = duet1.store[0]\n",
    "y_Train_Owner1_ptr = duet1.store[1]\n",
    "X_Test_Owner1_ptr = duet1.store[2]\n",
    "y_Test_Owner1_ptr = duet1.store[3]\n",
    "\n",
    "print(X_Train_Owner1_ptr.tags)\n",
    "print(X_Train_Owner1_ptr.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "81e8f725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Description</th>\n",
       "      <th>object_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;UID: 7083551ae5334fc99a22f4b43d44c9b7&gt;</td>\n",
       "      <td>[X_Train_Owner2]</td>\n",
       "      <td>Dataset of 400 samples, 7 feature</td>\n",
       "      <td>&lt;class 'torch.Tensor'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;UID: a45df775e0744a169e1fbbfa7fcbdc03&gt;</td>\n",
       "      <td>[y_Train_Owner2]</td>\n",
       "      <td>Dataset of 400 samples, 1 feature</td>\n",
       "      <td>&lt;class 'torch.Tensor'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;UID: 030c2da760bd4e9e8cd32f19c380988d&gt;</td>\n",
       "      <td>[X_Test_Owner2]</td>\n",
       "      <td>Dataset of 400 samples, 7 feature</td>\n",
       "      <td>&lt;class 'torch.Tensor'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;UID: 245b75ac7b1648f59352636b13d7044f&gt;</td>\n",
       "      <td>[y_Test_Owner2]</td>\n",
       "      <td>Dataset of 400 samples, 1 feature</td>\n",
       "      <td>&lt;class 'torch.Tensor'&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ID              Tags  \\\n",
       "0  <UID: 7083551ae5334fc99a22f4b43d44c9b7>  [X_Train_Owner2]   \n",
       "1  <UID: a45df775e0744a169e1fbbfa7fcbdc03>  [y_Train_Owner2]   \n",
       "2  <UID: 030c2da760bd4e9e8cd32f19c380988d>   [X_Test_Owner2]   \n",
       "3  <UID: 245b75ac7b1648f59352636b13d7044f>   [y_Test_Owner2]   \n",
       "\n",
       "                         Description             object_type  \n",
       "0  Dataset of 400 samples, 7 feature  <class 'torch.Tensor'>  \n",
       "1  Dataset of 400 samples, 1 feature  <class 'torch.Tensor'>  \n",
       "2  Dataset of 400 samples, 7 feature  <class 'torch.Tensor'>  \n",
       "3  Dataset of 400 samples, 1 feature  <class 'torch.Tensor'>  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duet2.store.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c0f5d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X_Train_Owner2']\n",
      "Dataset of 400 samples, 7 feature\n"
     ]
    }
   ],
   "source": [
    "X_Train_Owner2_ptr = duet2.store[0]\n",
    "y_Train_Owner2_ptr = duet2.store[1]\n",
    "X_Test_Owner2_ptr = duet2.store[2]\n",
    "y_Test_Owner2_ptr = duet2.store[3]\n",
    "\n",
    "print(X_Train_Owner2_ptr.tags)\n",
    "print(X_Train_Owner2_ptr.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd01d89",
   "metadata": {},
   "source": [
    "## Build a Base Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40a44ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyNet(sy.Module):\n",
    "    def __init__(self,input_channels,output_channels,torch_ref):\n",
    "        super(SyNet,self).__init__(torch_ref=torch_ref)\n",
    "        #Our network:\n",
    "        # Linear1->relu->Batchnorm->Linear2->relu->Batchnorm->Dropout->Linear3->output\n",
    "        # Softmax is added in the predict function\n",
    "        #This applies Linear transformation to input data. \n",
    "        self.fc1 = self.torch_ref.nn.Linear(input_channels,int(1.5*input_channels))\n",
    "        self.fc2 = self.torch_ref.nn.Linear(int(1.5*input_channels),int(1.5*input_channels))\n",
    "        self.fc3 = self.torch_ref.nn.Linear(int(1.5*input_channels),output_channels)\n",
    "        \n",
    "        self.relu = self.torch_ref.nn.ReLU()\n",
    "        self.dropout = self.torch_ref.nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = self.torch_ref.nn.BatchNorm1d(int(1.5*input_channels))\n",
    "        self.batchnorm2 = self.torch_ref.nn.BatchNorm1d(int(1.5*input_channels))\n",
    "        self.sigmoid = self.torch_ref.nn.Sigmoid()\n",
    "    #This must be implemented\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    def predict(self,x):\n",
    "        output = self.forward(x)\n",
    "        prediction = self.torch_ref.argmax(output,1)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "086f6a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_train(epochs, model, optimizer, X_ptr,y_ptr,criterion,torch_ref):\n",
    "    losses = []\n",
    "    for i in range(epochs):\n",
    "        #Precit the output for Given input\n",
    "        y_pred_ptr = model.forward(X_ptr)\n",
    "        #Compute Cross entropy loss\n",
    "        loss = criterion(y_pred_ptr,y_ptr)\n",
    "        loss_item = loss.item()\n",
    "        #Request to get the loss value\n",
    "        loss_value = loss_item.get(\n",
    "            reason=\"To evaluate training progress\",\n",
    "            request_block=True,\n",
    "            timeout_secs=5,\n",
    "        )\n",
    "        #Add loss to the list\n",
    "        losses.append(loss_value)\n",
    "        #Print loss\n",
    "        if i%50==0:\n",
    "            print(\"Epoch:\",i,\" Loss:\",loss_value)\n",
    "        \n",
    "        #Clear the previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        #Compute gradients\n",
    "        loss.backward()\n",
    "        #Adjust weights\n",
    "        optimizer.step()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e4539d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as base_torch         \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "14ecce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Classifier Model\n",
    "X_Train_Owner1_shape = X_Train_Owner1_ptr.shape.get(\n",
    "            reason=\"To evaluate training progress\",\n",
    "            request_block=True,\n",
    "            timeout_secs=5,\n",
    "        )\n",
    "input_channels = X_Train_Owner1_shape[1]\n",
    "output_channels = 2\n",
    "base_model = SyNet(input_channels,output_channels,base_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cc9031",
   "metadata": {},
   "source": [
    "## Send copy of model to Data Owner1and train them remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ca5409a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 0.6714057326316833\n",
      "Epoch: 50  Loss: 0.6365100145339966\n",
      "Epoch: 100  Loss: 0.6091839075088501\n",
      "Epoch: 150  Loss: 0.5800541043281555\n",
      "Epoch: 200  Loss: 0.5645753741264343\n",
      "Epoch: 250  Loss: 0.5452878475189209\n",
      "Epoch: 300  Loss: 0.5264979004859924\n",
      "Epoch: 350  Loss: 0.5114352107048035\n",
      "Epoch: 400  Loss: 0.49581339955329895\n",
      "Epoch: 450  Loss: 0.4737929105758667\n",
      "Epoch: 500  Loss: 0.46566444635391235\n",
      "Epoch: 550  Loss: 0.4550821781158447\n",
      "Epoch: 600  Loss: 0.4539676308631897\n",
      "Epoch: 650  Loss: 0.44768208265304565\n",
      "Epoch: 700  Loss: 0.4459100663661957\n",
      "Epoch: 750  Loss: 0.43706202507019043\n",
      "Epoch: 800  Loss: 0.4346407353878021\n",
      "Epoch: 850  Loss: 0.4331651031970978\n",
      "Epoch: 900  Loss: 0.42970994114875793\n",
      "Epoch: 950  Loss: 0.4294106960296631\n"
     ]
    }
   ],
   "source": [
    "remote_model1 = base_model.send(duet1)\n",
    "\n",
    "remote_torch1 = duet1.torch\n",
    "params = remote_model1.parameters()\n",
    "#Define loss criterion\n",
    "criterion = remote_torch1.nn.CrossEntropyLoss()\n",
    "#Define the optimizer\n",
    "optimizer = remote_torch1.optim.Adam(params=params, lr=0.001)\n",
    "#Number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Training\n",
    "losses = classifier_train(epochs, remote_model1, optimizer, X_Train_Owner1_ptr, y_Train_Owner1_ptr,criterion, remote_torch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5e04b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_Test_Owner1_predict_ptr = remote_model1.predict(X_Test_Owner1_ptr)\n",
    "#s = y_Test_Owner1_predict_ptr-y_Test_Owner1_ptr\n",
    "#res = s.get(reason=\"To evaluate training progress\",\n",
    "#             request_block=True,\n",
    "#             timeout_secs=5,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa7d6e5",
   "metadata": {},
   "source": [
    "## Send copy of model to Data Owner2and train them remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0af8c9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 0.6677805185317993\n",
      "Epoch: 50  Loss: 0.6347150206565857\n",
      "Epoch: 100  Loss: 0.6062013506889343\n",
      "Epoch: 150  Loss: 0.5813589692115784\n",
      "Epoch: 200  Loss: 0.5615498423576355\n",
      "Epoch: 250  Loss: 0.5425483584403992\n",
      "Epoch: 300  Loss: 0.5360767245292664\n",
      "Epoch: 350  Loss: 0.5218856334686279\n",
      "Epoch: 400  Loss: 0.5140787363052368\n",
      "Epoch: 450  Loss: 0.5067965984344482\n",
      "Epoch: 500  Loss: 0.5043188333511353\n",
      "Epoch: 550  Loss: 0.49677520990371704\n",
      "Epoch: 600  Loss: 0.4892673194408417\n",
      "Epoch: 650  Loss: 0.487337201833725\n",
      "Epoch: 700  Loss: 0.4887542426586151\n",
      "Epoch: 750  Loss: 0.48170891404151917\n",
      "Epoch: 800  Loss: 0.4763626754283905\n",
      "Epoch: 850  Loss: 0.4875952899456024\n",
      "Epoch: 900  Loss: 0.47301235795021057\n",
      "Epoch: 950  Loss: 0.47354790568351746\n"
     ]
    }
   ],
   "source": [
    "remote_model2 = base_model.send(duet2)\n",
    "\n",
    "remote_torch2 = duet2.torch\n",
    "params = remote_model2.parameters()\n",
    "#Define loss criterion\n",
    "criterion = remote_torch2.nn.CrossEntropyLoss()\n",
    "#Define the optimizer\n",
    "optimizer = remote_torch2.optim.Adam(params=params, lr=0.001)\n",
    "#Number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Training\n",
    "losses = classifier_train(epochs, remote_model2, optimizer, X_Train_Owner2_ptr, y_Train_Owner2_ptr,criterion, remote_torch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b257d3d",
   "metadata": {},
   "source": [
    "## Averaging Model Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bd7debc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d95faab7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model parameters:\n",
      "[Parameter containing:\n",
      "tensor([[ 0.1080,  0.3046,  0.2469, -0.1963, -0.1279,  0.2022, -0.2835],\n",
      "        [-0.3687, -0.0809,  0.2325,  0.2654, -0.0539,  0.3263,  0.0971],\n",
      "        [ 0.1901, -0.0424,  0.1180, -0.2705,  0.0703,  0.2696, -0.3205],\n",
      "        [ 0.1972, -0.1518,  0.1409,  0.3067, -0.1083,  0.1113, -0.0007],\n",
      "        [ 0.1251,  0.1007, -0.2291,  0.0200,  0.3618,  0.1570,  0.1333],\n",
      "        [-0.2548, -0.0458,  0.1391, -0.1971,  0.3332, -0.0134,  0.0690],\n",
      "        [ 0.3161,  0.1424,  0.2739, -0.3420,  0.1806,  0.0810, -0.0678],\n",
      "        [-0.3193, -0.2309, -0.2773,  0.3722, -0.1205, -0.1357, -0.0206],\n",
      "        [-0.0185, -0.3740,  0.0416, -0.2869, -0.0853,  0.0132,  0.1743],\n",
      "        [ 0.3443, -0.3391,  0.0027,  0.0313, -0.3370, -0.2535,  0.2128]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-2.1954e-01, -2.5332e-04, -2.3299e-02, -3.5002e-01,  3.7355e-01,\n",
      "        -2.7745e-01, -1.8291e-01, -2.7888e-01, -1.0975e-01, -1.5595e-01],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 2.9254e-04, -1.4380e-01, -1.1766e-01,  1.3625e-01, -3.9160e-02,\n",
      "          8.3956e-02, -7.2507e-02,  4.1197e-02,  8.0424e-02, -2.0596e-01],\n",
      "        [-1.6183e-01,  8.6125e-02, -3.7077e-02,  2.5805e-01,  1.7157e-01,\n",
      "          7.7031e-02,  1.7547e-01,  1.7720e-01, -2.5898e-01, -6.8953e-02],\n",
      "        [ 2.6150e-01, -2.5623e-01, -1.2110e-01,  2.1839e-01, -1.3314e-01,\n",
      "         -4.3169e-03, -1.5648e-01, -4.8231e-02, -1.8144e-01, -7.8337e-02],\n",
      "        [-2.5367e-01,  1.7586e-01,  1.4294e-01, -2.7222e-01, -2.9411e-02,\n",
      "         -6.2317e-02,  2.7071e-01,  2.2172e-01, -5.0410e-02, -5.3343e-02],\n",
      "        [ 1.5383e-01, -3.0383e-01, -2.0409e-01, -6.0892e-02, -2.6365e-01,\n",
      "         -5.3577e-02, -2.2169e-01, -1.4725e-01, -8.5292e-03, -5.5901e-02],\n",
      "        [ 9.9884e-02,  3.0133e-01, -5.7187e-03, -2.2805e-01, -2.3236e-01,\n",
      "          1.5446e-02, -1.0860e-01,  1.9912e-01, -3.0655e-01, -2.2881e-01],\n",
      "        [ 2.6999e-02,  1.0581e-01,  5.3306e-02,  1.9534e-01, -1.3843e-01,\n",
      "          1.7697e-01,  1.1826e-01,  1.1314e-01, -1.8676e-01,  2.4568e-02],\n",
      "        [-9.1793e-02,  1.1210e-01,  1.3526e-01,  1.9097e-01, -2.3444e-01,\n",
      "         -1.2810e-01, -2.0508e-01,  6.7177e-02,  2.7908e-01,  1.1942e-03],\n",
      "        [ 2.8501e-01,  8.8302e-02, -1.8576e-01,  2.8509e-01, -4.8814e-02,\n",
      "          6.6271e-02, -1.3607e-01,  1.5460e-02,  1.4819e-01, -1.6712e-01],\n",
      "        [ 1.6286e-01,  2.4423e-01,  2.3162e-01,  2.0031e-02, -1.1714e-02,\n",
      "          2.9630e-01, -6.8546e-02,  1.9280e-02, -5.4335e-02, -5.7498e-02]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1021,  0.1934, -0.0957, -0.1362,  0.1950, -0.0785, -0.2240, -0.0994,\n",
      "        -0.0802, -0.2770], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0492, -0.0137, -0.1105,  0.2753,  0.1784,  0.0777, -0.0134, -0.0267,\n",
      "         -0.1955, -0.0912],\n",
      "        [ 0.1163, -0.2211, -0.2440,  0.0297, -0.0716, -0.2728,  0.1193, -0.0784,\n",
      "         -0.2011, -0.2215]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3017,  0.1393], requires_grad=True), Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True), Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)]\n",
      "\n",
      "Remote model1 parameters:\n",
      "[Parameter containing:\n",
      "tensor([[ 0.0779,  0.0569,  0.2021, -0.1684, -0.0360,  0.2680, -0.1703],\n",
      "        [-0.2216, -0.1781,  0.1602,  0.0738, -0.0785,  0.3156,  0.2587],\n",
      "        [ 0.1028, -0.0767, -0.0728, -0.2424,  0.0311,  0.3255, -0.2238],\n",
      "        [ 0.2700, -0.2687,  0.2896,  0.1487,  0.0590, -0.2297,  0.0506],\n",
      "        [ 0.0889, -0.0486,  0.0543,  0.0124,  0.5594,  0.1218,  0.0915],\n",
      "        [-0.2139,  0.1943, -0.0509,  0.1414,  0.2502, -0.2040,  0.1275],\n",
      "        [ 0.1308,  0.0856,  0.3098, -0.1076,  0.2109,  0.0107, -0.1479],\n",
      "        [ 0.1113, -0.3909, -0.0487,  0.1316,  0.0051, -0.0901,  0.3607],\n",
      "        [-0.1767, -0.1866, -0.1866, -0.3488, -0.0737,  0.1476,  0.0562],\n",
      "        [ 0.3401, -0.3482,  0.0943, -0.0443, -0.2453, -0.2647,  0.0708]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.4654, -0.3129,  0.3822, -0.0754,  0.4590, -0.3758, -0.4711, -0.0207,\n",
      "        -0.2039, -0.2992], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0737, -0.0900, -0.0814,  0.0586,  0.0841, -0.0670, -0.3504, -0.0583,\n",
      "         -0.0751, -0.0967],\n",
      "        [-0.2510,  0.1492,  0.2701,  0.0441, -0.0974,  0.0936,  0.1519,  0.0801,\n",
      "         -0.2349, -0.0692],\n",
      "        [ 0.0672, -0.3336,  0.0641,  0.2866, -0.1234,  0.0381, -0.0717, -0.3511,\n",
      "          0.0516, -0.2669],\n",
      "        [-0.3300,  0.1911,  0.3288, -0.0041, -0.1240,  0.1240,  0.2102,  0.0782,\n",
      "         -0.2733, -0.0470],\n",
      "        [ 0.1824, -0.2100, -0.1797,  0.1621, -0.2804, -0.0671, -0.0362, -0.0280,\n",
      "          0.1604, -0.0600],\n",
      "        [-0.3272,  0.1681,  0.3080, -0.0707, -0.1110,  0.1088,  0.2033,  0.1018,\n",
      "         -0.2651, -0.0708],\n",
      "        [ 0.0085, -0.2439,  0.0581, -0.1347, -0.0850,  0.0041, -0.2232,  0.0349,\n",
      "         -0.2861,  0.1718],\n",
      "        [ 0.0352, -0.0728,  0.0597,  0.1329, -0.5857, -0.0027,  0.0555,  0.0770,\n",
      "          0.0827, -0.1331],\n",
      "        [-0.0259,  0.1564,  0.2658,  0.2376, -0.1146,  0.2158,  0.0220,  0.1460,\n",
      "         -0.0675, -0.2506],\n",
      "        [-0.1895,  0.0127,  0.2952,  0.1333, -0.1547,  0.0118,  0.1253,  0.1980,\n",
      "         -0.1833, -0.4108]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0573, -0.1389, -0.3736, -0.2304, -0.3238, -0.2568, -0.0869, -0.2712,\n",
      "         0.0082, -0.2306], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.4357,  0.5446,  0.5617,  0.8719,  0.7248,  0.7890, -0.4469,  0.6150,\n",
      "          0.3382,  0.5159],\n",
      "        [ 0.4773, -0.7407, -0.7082, -0.5927, -0.7301, -0.9877,  0.4444, -0.7137,\n",
      "         -0.4937, -0.6992]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.5634,  0.3770], requires_grad=True), Parameter containing:\n",
      "tensor([1.0303, 0.9088, 1.1634, 0.8478, 1.1169, 0.9440, 1.0075, 0.8814, 0.9531,\n",
      "        0.9987], requires_grad=True), Parameter containing:\n",
      "tensor([-0.1320,  0.0729,  0.0211, -0.2541,  0.2822, -0.2318, -0.0502,  0.0968,\n",
      "         0.0206,  0.2870], requires_grad=True), Parameter containing:\n",
      "tensor([1.4533, 1.8014, 1.7923, 1.8180, 1.8131, 1.9640, 1.5331, 1.9992, 1.4732,\n",
      "        1.8331], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.3638, -0.3523, -0.4388, -0.3546, -0.3349, -0.3554,  0.4271, -0.4251,\n",
      "        -0.4629, -0.3775], requires_grad=True)]\n",
      "\n",
      "Remote model2 parameters:\n",
      "[Parameter containing:\n",
      "tensor([[ 0.2805,  0.2078,  0.2659, -0.0736, -0.1933,  0.1437, -0.3040],\n",
      "        [-0.5771, -0.1384, -0.0928,  0.1353, -0.1181,  0.3780,  0.0328],\n",
      "        [ 0.2520, -0.1970,  0.2078, -0.3847,  0.0600, -0.0079, -0.1552],\n",
      "        [ 0.0747, -0.2829,  0.1337,  0.3658, -0.1159,  0.0539,  0.0621],\n",
      "        [-0.1712,  0.1728, -0.1325, -0.1952,  0.2034,  0.0551,  0.1286],\n",
      "        [-0.2561,  0.2618, -0.0456, -0.2311,  0.2943, -0.0321,  0.0985],\n",
      "        [ 0.2760,  0.1936,  0.2807, -0.4258,  0.0132,  0.1108, -0.1365],\n",
      "        [-0.0857, -0.3008, -0.0978,  0.5168, -0.0946, -0.0879, -0.1025],\n",
      "        [ 0.2193, -0.2837,  0.1380, -0.2875, -0.0462, -0.0602,  0.1090],\n",
      "        [ 0.2251, -0.2831,  0.2760,  0.1932, -0.2166, -0.0986,  0.1936]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.1767, -0.1235,  0.0946, -0.2080,  0.5507, -0.3346, -0.2749, -0.3276,\n",
      "        -0.2811, -0.1538], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0103, -0.0584, -0.1361,  0.1914, -0.0872,  0.0305,  0.0477, -0.1753,\n",
      "          0.0123, -0.0256],\n",
      "        [-0.0613,  0.1124,  0.2222, -0.0234,  0.1416, -0.0799,  0.0389,  0.2818,\n",
      "         -0.2452, -0.2141],\n",
      "        [ 0.1434, -0.2482, -0.0537,  0.1184,  0.1048, -0.0424, -0.3370,  0.0680,\n",
      "          0.0464, -0.2748],\n",
      "        [-0.3380,  0.0268,  0.1403, -0.2089,  0.0871, -0.1645,  0.1802,  0.1146,\n",
      "         -0.1359,  0.1231],\n",
      "        [ 0.1753,  0.1053,  0.1643,  0.0725, -0.0664,  0.0981, -0.3507,  0.0587,\n",
      "          0.1120, -0.3780],\n",
      "        [ 0.1083,  0.2210,  0.2236, -0.2224, -0.1933,  0.1559, -0.2574,  0.0789,\n",
      "         -0.4961, -0.1036],\n",
      "        [ 0.1032,  0.1153, -0.0448,  0.0911, -0.2463,  0.0874,  0.1799, -0.0548,\n",
      "         -0.1049,  0.0198],\n",
      "        [ 0.0785,  0.1969,  0.1818, -0.0700, -0.0541,  0.1005, -0.2047,  0.1682,\n",
      "          0.1052, -0.2659],\n",
      "        [ 0.2849,  0.1111,  0.1126,  0.1057,  0.0336,  0.1010, -0.3881,  0.1373,\n",
      "         -0.0445, -0.2845],\n",
      "        [ 0.0643,  0.1209,  0.2737, -0.1397,  0.0318,  0.0577, -0.2560,  0.1444,\n",
      "         -0.5719, -0.0068]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1978,  0.0551, -0.3437, -0.2435, -0.0104, -0.1824, -0.4122, -0.0812,\n",
      "        -0.0633, -0.1803], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.5809,  0.4660,  0.7143,  0.8588,  0.7094,  0.5998, -0.4096,  0.5536,\n",
      "          0.3764,  0.4845],\n",
      "        [ 0.6459, -0.5647, -0.8506, -0.7529, -0.5218, -0.6836,  0.5837, -0.6367,\n",
      "         -0.6113, -0.6678]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4908,  0.3193], requires_grad=True), Parameter containing:\n",
      "tensor([1.0325, 0.9581, 1.0242, 0.7922, 0.8555, 0.9981, 1.1886, 1.0343, 1.1050,\n",
      "        1.2288], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0560,  0.0759,  0.0061, -0.0112,  0.1390,  0.0976,  0.1680, -0.0982,\n",
      "         0.1233,  0.2288], requires_grad=True), Parameter containing:\n",
      "tensor([1.7405, 1.5988, 1.8930, 1.9443, 1.7346, 1.6288, 1.7755, 1.8671, 1.7644,\n",
      "        1.8388], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.3037, -0.2378, -0.1842, -0.2139, -0.1702, -0.2150,  0.2445, -0.2621,\n",
      "        -0.3162, -0.2496], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "param1 = remote_model1.parameters().get(request_block=True)\n",
    "param2 = remote_model2.parameters().get(request_block=True)\n",
    "\n",
    "print(\"Base Model parameters:\")\n",
    "print(base_model.parameters())\n",
    "print()\n",
    "\n",
    "print(\"Remote model1 parameters:\")\n",
    "print(param1)\n",
    "print()\n",
    "\n",
    "print(\"Remote model2 parameters:\")\n",
    "print(param2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dd6b64dd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('fc1.weight', tensor([[ 0.0779,  0.0569,  0.2021, -0.1684, -0.0360,  0.2680, -0.1703],\n",
      "        [-0.2216, -0.1781,  0.1602,  0.0738, -0.0785,  0.3156,  0.2587],\n",
      "        [ 0.1028, -0.0767, -0.0728, -0.2424,  0.0311,  0.3255, -0.2238],\n",
      "        [ 0.2700, -0.2687,  0.2896,  0.1487,  0.0590, -0.2297,  0.0506],\n",
      "        [ 0.0889, -0.0486,  0.0543,  0.0124,  0.5594,  0.1218,  0.0915],\n",
      "        [-0.2139,  0.1943, -0.0509,  0.1414,  0.2502, -0.2040,  0.1275],\n",
      "        [ 0.1308,  0.0856,  0.3098, -0.1076,  0.2109,  0.0107, -0.1479],\n",
      "        [ 0.1113, -0.3909, -0.0487,  0.1316,  0.0051, -0.0901,  0.3607],\n",
      "        [-0.1767, -0.1866, -0.1866, -0.3488, -0.0737,  0.1476,  0.0562],\n",
      "        [ 0.3401, -0.3482,  0.0943, -0.0443, -0.2453, -0.2647,  0.0708]])), ('fc1.bias', tensor([-0.4654, -0.3129,  0.3822, -0.0754,  0.4590, -0.3758, -0.4711, -0.0207,\n",
      "        -0.2039, -0.2992])), ('fc2.weight', tensor([[ 0.0737, -0.0900, -0.0814,  0.0586,  0.0841, -0.0670, -0.3504, -0.0583,\n",
      "         -0.0751, -0.0967],\n",
      "        [-0.2510,  0.1492,  0.2701,  0.0441, -0.0974,  0.0936,  0.1519,  0.0801,\n",
      "         -0.2349, -0.0692],\n",
      "        [ 0.0672, -0.3336,  0.0641,  0.2866, -0.1234,  0.0381, -0.0717, -0.3511,\n",
      "          0.0516, -0.2669],\n",
      "        [-0.3300,  0.1911,  0.3288, -0.0041, -0.1240,  0.1240,  0.2102,  0.0782,\n",
      "         -0.2733, -0.0470],\n",
      "        [ 0.1824, -0.2100, -0.1797,  0.1621, -0.2804, -0.0671, -0.0362, -0.0280,\n",
      "          0.1604, -0.0600],\n",
      "        [-0.3272,  0.1681,  0.3080, -0.0707, -0.1110,  0.1088,  0.2033,  0.1018,\n",
      "         -0.2651, -0.0708],\n",
      "        [ 0.0085, -0.2439,  0.0581, -0.1347, -0.0850,  0.0041, -0.2232,  0.0349,\n",
      "         -0.2861,  0.1718],\n",
      "        [ 0.0352, -0.0728,  0.0597,  0.1329, -0.5857, -0.0027,  0.0555,  0.0770,\n",
      "          0.0827, -0.1331],\n",
      "        [-0.0259,  0.1564,  0.2658,  0.2376, -0.1146,  0.2158,  0.0220,  0.1460,\n",
      "         -0.0675, -0.2506],\n",
      "        [-0.1895,  0.0127,  0.2952,  0.1333, -0.1547,  0.0118,  0.1253,  0.1980,\n",
      "         -0.1833, -0.4108]])), ('fc2.bias', tensor([ 0.0573, -0.1389, -0.3736, -0.2304, -0.3238, -0.2568, -0.0869, -0.2712,\n",
      "         0.0082, -0.2306])), ('fc3.weight', tensor([[-0.4357,  0.5446,  0.5617,  0.8719,  0.7248,  0.7890, -0.4469,  0.6150,\n",
      "          0.3382,  0.5159],\n",
      "        [ 0.4773, -0.7407, -0.7082, -0.5927, -0.7301, -0.9877,  0.4444, -0.7137,\n",
      "         -0.4937, -0.6992]])), ('fc3.bias', tensor([-0.5634,  0.3770])), ('batchnorm1.weight', tensor([1.0303, 0.9088, 1.1634, 0.8478, 1.1169, 0.9440, 1.0075, 0.8814, 0.9531,\n",
      "        0.9987])), ('batchnorm1.bias', tensor([-0.1320,  0.0729,  0.0211, -0.2541,  0.2822, -0.2318, -0.0502,  0.0968,\n",
      "         0.0206,  0.2870])), ('batchnorm1.running_mean', tensor([0.0452, 0.0948, 0.3944, 0.1725, 0.4608, 0.0297, 0.0712, 0.2397, 0.0863,\n",
      "        0.1297])), ('batchnorm1.running_var', tensor([0.0237, 0.0445, 0.1879, 0.0710, 0.3231, 0.0093, 0.0480, 0.1230, 0.0278,\n",
      "        0.0630])), ('batchnorm1.num_batches_tracked', tensor(1014)), ('batchnorm2.weight', tensor([1.4533, 1.8014, 1.7923, 1.8180, 1.8131, 1.9640, 1.5331, 1.9992, 1.4732,\n",
      "        1.8331])), ('batchnorm2.bias', tensor([ 0.3638, -0.3523, -0.4388, -0.3546, -0.3349, -0.3554,  0.4271, -0.4251,\n",
      "        -0.4629, -0.3775])), ('batchnorm2.running_mean', tensor([0.1360, 0.0354, 0.0008, 0.0361, 0.0068, 0.0308, 0.1616, 0.0065, 0.0652,\n",
      "        0.0088])), ('batchnorm2.running_var', tensor([0.0153, 0.0086, 0.0001, 0.0100, 0.0008, 0.0073, 0.0283, 0.0013, 0.0297,\n",
      "        0.0020])), ('batchnorm2.num_batches_tracked', tensor(1014))])\n",
      "OrderedDict([('fc1.weight', tensor([[ 0.2805,  0.2078,  0.2659, -0.0736, -0.1933,  0.1437, -0.3040],\n",
      "        [-0.5771, -0.1384, -0.0928,  0.1353, -0.1181,  0.3780,  0.0328],\n",
      "        [ 0.2520, -0.1970,  0.2078, -0.3847,  0.0600, -0.0079, -0.1552],\n",
      "        [ 0.0747, -0.2829,  0.1337,  0.3658, -0.1159,  0.0539,  0.0621],\n",
      "        [-0.1712,  0.1728, -0.1325, -0.1952,  0.2034,  0.0551,  0.1286],\n",
      "        [-0.2561,  0.2618, -0.0456, -0.2311,  0.2943, -0.0321,  0.0985],\n",
      "        [ 0.2760,  0.1936,  0.2807, -0.4258,  0.0132,  0.1108, -0.1365],\n",
      "        [-0.0857, -0.3008, -0.0978,  0.5168, -0.0946, -0.0879, -0.1025],\n",
      "        [ 0.2193, -0.2837,  0.1380, -0.2875, -0.0462, -0.0602,  0.1090],\n",
      "        [ 0.2251, -0.2831,  0.2760,  0.1932, -0.2166, -0.0986,  0.1936]])), ('fc1.bias', tensor([-0.1767, -0.1235,  0.0946, -0.2080,  0.5507, -0.3346, -0.2749, -0.3276,\n",
      "        -0.2811, -0.1538])), ('fc2.weight', tensor([[-0.0103, -0.0584, -0.1361,  0.1914, -0.0872,  0.0305,  0.0477, -0.1753,\n",
      "          0.0123, -0.0256],\n",
      "        [-0.0613,  0.1124,  0.2222, -0.0234,  0.1416, -0.0799,  0.0389,  0.2818,\n",
      "         -0.2452, -0.2141],\n",
      "        [ 0.1434, -0.2482, -0.0537,  0.1184,  0.1048, -0.0424, -0.3370,  0.0680,\n",
      "          0.0464, -0.2748],\n",
      "        [-0.3380,  0.0268,  0.1403, -0.2089,  0.0871, -0.1645,  0.1802,  0.1146,\n",
      "         -0.1359,  0.1231],\n",
      "        [ 0.1753,  0.1053,  0.1643,  0.0725, -0.0664,  0.0981, -0.3507,  0.0587,\n",
      "          0.1120, -0.3780],\n",
      "        [ 0.1083,  0.2210,  0.2236, -0.2224, -0.1933,  0.1559, -0.2574,  0.0789,\n",
      "         -0.4961, -0.1036],\n",
      "        [ 0.1032,  0.1153, -0.0448,  0.0911, -0.2463,  0.0874,  0.1799, -0.0548,\n",
      "         -0.1049,  0.0198],\n",
      "        [ 0.0785,  0.1969,  0.1818, -0.0700, -0.0541,  0.1005, -0.2047,  0.1682,\n",
      "          0.1052, -0.2659],\n",
      "        [ 0.2849,  0.1111,  0.1126,  0.1057,  0.0336,  0.1010, -0.3881,  0.1373,\n",
      "         -0.0445, -0.2845],\n",
      "        [ 0.0643,  0.1209,  0.2737, -0.1397,  0.0318,  0.0577, -0.2560,  0.1444,\n",
      "         -0.5719, -0.0068]])), ('fc2.bias', tensor([ 0.1978,  0.0551, -0.3437, -0.2435, -0.0104, -0.1824, -0.4122, -0.0812,\n",
      "        -0.0633, -0.1803])), ('fc3.weight', tensor([[-0.5809,  0.4660,  0.7143,  0.8588,  0.7094,  0.5998, -0.4096,  0.5536,\n",
      "          0.3764,  0.4845],\n",
      "        [ 0.6459, -0.5647, -0.8506, -0.7529, -0.5218, -0.6836,  0.5837, -0.6367,\n",
      "         -0.6113, -0.6678]])), ('fc3.bias', tensor([-0.4908,  0.3193])), ('batchnorm1.weight', tensor([1.0325, 0.9581, 1.0242, 0.7922, 0.8555, 0.9981, 1.1886, 1.0343, 1.1050,\n",
      "        1.2288])), ('batchnorm1.bias', tensor([-0.0560,  0.0759,  0.0061, -0.0112,  0.1390,  0.0976,  0.1680, -0.0982,\n",
      "         0.1233,  0.2288])), ('batchnorm1.running_mean', tensor([0.1592, 0.2396, 0.2730, 0.1360, 0.5664, 0.0752, 0.1881, 0.1782, 0.0827,\n",
      "        0.1613])), ('batchnorm1.running_var', tensor([0.0800, 0.1210, 0.1500, 0.0706, 0.1416, 0.0363, 0.1385, 0.1275, 0.0409,\n",
      "        0.0943])), ('batchnorm1.num_batches_tracked', tensor(1000)), ('batchnorm2.weight', tensor([1.7405, 1.5988, 1.8930, 1.9443, 1.7346, 1.6288, 1.7755, 1.8671, 1.7644,\n",
      "        1.8388])), ('batchnorm2.bias', tensor([ 0.3037, -0.2378, -0.1842, -0.2139, -0.1702, -0.2150,  0.2445, -0.2621,\n",
      "        -0.3162, -0.2496])), ('batchnorm2.running_mean', tensor([0.2057, 0.1463, 0.0010, 0.0105, 0.0616, 0.0588, 0.0190, 0.0603, 0.0641,\n",
      "        0.0422])), ('batchnorm2.running_var', tensor([1.1637e-02, 5.5441e-02, 2.7988e-05, 1.0066e-03, 1.8247e-02, 1.8971e-02,\n",
      "        4.4351e-03, 2.6549e-02, 2.3655e-02, 1.0559e-02])), ('batchnorm2.num_batches_tracked', tensor(1000))])\n"
     ]
    }
   ],
   "source": [
    "remote_model1_updates = remote_model1.get(\n",
    "    request_block=True\n",
    ").state_dict()\n",
    "\n",
    "print(remote_model1_updates)\n",
    "\n",
    "remote_model2_updates = remote_model2.get(\n",
    "    request_block=True\n",
    ").state_dict()\n",
    "\n",
    "print(remote_model2_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "95d18df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Average Model\n",
    "avg_updates = OrderedDict()\n",
    "for key in remote_model1_updates.keys():\n",
    "    avg_updates[key] = (remote_model1_updates[key] + remote_model2_updates[key]) / 2\n",
    "\n",
    "    \n",
    "combined_model = SyNet(input_channels,output_channels,base_torch)\n",
    "\n",
    "combined_model.load_state_dict(avg_updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5ac0b2",
   "metadata": {},
   "source": [
    "## Test the prediction of combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f0ee305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "german = pd.read_csv(\"./data/german_data_2.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4666561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.20\n",
    "\n",
    "processed_data = None\n",
    "categorical = None\n",
    "label_encoders = {}\n",
    "\n",
    "def preprocessing(dataset, data, test_size):\n",
    "    \"\"\"\n",
    "    Preprocess dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: DataFrame\n",
    "        Pandas dataframe containing German dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    global processed_data\n",
    "    global categorical\n",
    "    global label_encoders\n",
    "\n",
    "    # Reset global variables\n",
    "    \n",
    "    processed_data = None\n",
    "    categorical = None\n",
    "    label_encoders = {}\n",
    "\n",
    "\n",
    "    if dataset == \"German\":\n",
    "        # Drop savings account and checkings account columns as they contain a lot\n",
    "        # of NaN values and may not always be available in real life scenarios\n",
    "        data = data.drop(columns = ['Saving accounts', 'Checking account'])\n",
    "        \n",
    "    dat_dict = data.to_dict()\n",
    "    new_dat_dict = {}\n",
    "\n",
    "    # rename columns(Make them lowercase and snakecase)\n",
    "    for key, value in dat_dict.items():\n",
    "        newKey = key\n",
    "        if type(key) == str:\n",
    "            newKey = newKey.lower().replace(' ', '_')\n",
    "        # if newKey != key:\n",
    "        new_dat_dict[newKey] = dat_dict[key]\n",
    "    del dat_dict\n",
    "\n",
    "    data = pd.DataFrame.from_dict(new_dat_dict)\n",
    "    del new_dat_dict\n",
    "\n",
    "\n",
    "    # print(data.describe())\n",
    "    # print(data.describe(include='O'))\n",
    "\n",
    "    cols = data.columns\n",
    "    num_cols = data._get_numeric_data().columns\n",
    "    categorical = list(set(cols) - set(num_cols))\n",
    "\n",
    "    # Drop null rows\n",
    "    data = data.dropna()\n",
    "\n",
    "    # Encode text columns to number values\n",
    "    for category in categorical:\n",
    "        le = LabelEncoder()\n",
    "        data[category] = le.fit_transform(data[category])\n",
    "        label_encoders[category] = le\n",
    "\n",
    "    for col in data.columns:\n",
    "        if(col not in categorical):\n",
    "            data[col] = (data[col].astype('float') - np.mean(data[col].astype('float')))/np.std(data[col].astype('float'))\n",
    "\n",
    "    # print(data.describe())\n",
    "    # print(data.describe(include='O'))\n",
    "\n",
    "    processed_data = data\n",
    "\n",
    "    # Get Training parameters\n",
    "    if dataset == \"German\":\n",
    "        target_col = data.columns[-1]\n",
    "        x = data.drop(columns=target_col, axis=1)\n",
    "        y = data[target_col].astype('int')\n",
    "    elif dataset == \"Australian\":\n",
    "        x = data.drop(14, axis=1)\n",
    "        y = data[14].astype('int')\n",
    "    elif dataset == \"Japanese\":\n",
    "        x = data.drop(15, axis=1)\n",
    "        y = data[15].astype('int')\n",
    "    elif dataset == \"Taiwan\":\n",
    "        x = data.drop('default_payment_next_month', axis=1)\n",
    "        y = data['default_payment_next_month'].astype('int')\n",
    "    elif dataset == \"Polish\":\n",
    "        x = data.drop('class', axis=1)\n",
    "        y = data['class'].astype('int')\n",
    "\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = test_size)\n",
    "    x_train = pd.DataFrame(x_train)\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    x_train = sc.fit_transform(x_train)\n",
    "    x_test = sc.transform(x_test)\n",
    "    \n",
    "    y_train = y_train[y_train.columns[0]].to_numpy()\n",
    "    y_test = y_test[y_test.columns[0]].to_numpy()\n",
    "\n",
    "    return (x_train, x_test, y_train, y_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocessing(\"German\", german, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c1374a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trnasform the input to tensor\n",
    "X_test_tensor = base_torch.FloatTensor(X_test)\n",
    "y_test_tensor = base_torch.tensor(y_test,dtype=base_torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7a7626dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcualte Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def accuracy(model,X,y):\n",
    "    print(accuracy_score(model.predict(X),y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c094a5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69\n"
     ]
    }
   ],
   "source": [
    "accuracy(combined_model,X_test_tensor,y_test_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
