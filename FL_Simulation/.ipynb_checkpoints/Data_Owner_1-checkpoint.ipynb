{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ccb753e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "import torch as t\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080121c6",
   "metadata": {},
   "source": [
    "## Launch a Duet Sever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1259c0ab",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤  ðŸŽ¸  â™ªâ™ªâ™ª Starting Duet â™«â™«â™«  ðŸŽ»  ðŸŽ¹\n",
      "\n",
      "â™«â™«â™« >\u001b[93m DISCLAIMER\u001b[0m: \u001b[1mDuet is an experimental feature currently in beta.\n",
      "â™«â™«â™« > Use at your own risk.\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "    > â¤ï¸ \u001b[91mLove\u001b[0m \u001b[92mDuet\u001b[0m? \u001b[93mPlease\u001b[0m \u001b[94mconsider\u001b[0m \u001b[95msupporting\u001b[0m \u001b[91mour\u001b[0m \u001b[93mcommunity!\u001b[0m\n",
      "    > https://github.com/sponsors/OpenMined\u001b[1m\n",
      "\n",
      "â™«â™«â™« > Punching through firewall to OpenGrid Network Node at:\n",
      "â™«â™«â™« > http://ec2-18-218-7-180.us-east-2.compute.amazonaws.com:5000\n",
      "â™«â™«â™« >\n",
      "â™«â™«â™« > ...waiting for response from OpenGrid Network... \n",
      "â™«â™«â™« > \u001b[92mDONE!\u001b[0m\n",
      "\n",
      "â™«â™«â™« > \u001b[95mSTEP 1:\u001b[0m Send the following code to your Duet Partner!\n",
      "\n",
      "import syft as sy\n",
      "duet = sy.join_duet(loopback=True)\n",
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "failed to load client ID",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m duet \u001b[38;5;241m=\u001b[39m \u001b[43msy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch_duet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloopback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\syft\\grid\\duet\\__init__.py:214\u001b[0m, in \u001b[0;36mlaunch_duet\u001b[1;34m(logging, network_url, loopback, credential_exchanger, db_path)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loopback:\n\u001b[0;32m    213\u001b[0m     credential_exchanger \u001b[38;5;241m=\u001b[39m OpenGridTokenFileExchanger()\n\u001b[1;32m--> 214\u001b[0m target_id \u001b[38;5;241m=\u001b[39m \u001b[43mcredential_exchanger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcredential\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignaling_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mduet_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâ™«â™«â™« > Connecting...\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mprint\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    218\u001b[0m _ \u001b[38;5;241m=\u001b[39m WebRTCDuet(\n\u001b[0;32m    219\u001b[0m     node\u001b[38;5;241m=\u001b[39mmy_domain,\n\u001b[0;32m    220\u001b[0m     target_id\u001b[38;5;241m=\u001b[39mtarget_id,\n\u001b[0;32m    221\u001b[0m     signaling_client\u001b[38;5;241m=\u001b[39msignaling_client,\n\u001b[0;32m    222\u001b[0m     offer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    223\u001b[0m )\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\syft\\grid\\duet\\exchange_ids.py:125\u001b[0m, in \u001b[0;36mOpenGridTokenFileExchanger.run\u001b[1;34m(self, credential)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_exchange(credential\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcredential)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_server_exchange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcredential\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcredential\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\syft\\grid\\duet\\exchange_ids.py:167\u001b[0m, in \u001b[0;36mOpenGridTokenFileExchanger._server_exchange\u001b[1;34m(self, credential)\u001b[0m\n\u001b[0;32m    164\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed to load client ID\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m client_id\n",
      "\u001b[1;31mException\u001b[0m: failed to load client ID"
     ]
    }
   ],
   "source": [
    "duet = sy.launch_duet(loopback = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734f8ab1",
   "metadata": {},
   "source": [
    "## Read Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10152986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "german = pd.read_csv(\"./data/german_data_1.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8c4c4829",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.20\n",
    "\n",
    "processed_data = None\n",
    "categorical = None\n",
    "label_encoders = {}\n",
    "\n",
    "def preprocessing(dataset, data, test_size):\n",
    "    \"\"\"\n",
    "    Preprocess dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: DataFrame\n",
    "        Pandas dataframe containing German dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    global processed_data\n",
    "    global categorical\n",
    "    global label_encoders\n",
    "\n",
    "    # Reset global variables\n",
    "    \n",
    "    processed_data = None\n",
    "    categorical = None\n",
    "    label_encoders = {}\n",
    "\n",
    "\n",
    "    if dataset == \"German\":\n",
    "        # Drop savings account and checkings account columns as they contain a lot\n",
    "        # of NaN values and may not always be available in real life scenarios\n",
    "        data = data.drop(columns = ['Saving accounts', 'Checking account'])\n",
    "        \n",
    "    dat_dict = data.to_dict()\n",
    "    new_dat_dict = {}\n",
    "\n",
    "    # rename columns(Make them lowercase and snakecase)\n",
    "    for key, value in dat_dict.items():\n",
    "        newKey = key\n",
    "        if type(key) == str:\n",
    "            newKey = newKey.lower().replace(' ', '_')\n",
    "        # if newKey != key:\n",
    "        new_dat_dict[newKey] = dat_dict[key]\n",
    "    del dat_dict\n",
    "\n",
    "    data = pd.DataFrame.from_dict(new_dat_dict)\n",
    "    del new_dat_dict\n",
    "\n",
    "\n",
    "    # print(data.describe())\n",
    "    # print(data.describe(include='O'))\n",
    "\n",
    "    cols = data.columns\n",
    "    num_cols = data._get_numeric_data().columns\n",
    "    categorical = list(set(cols) - set(num_cols))\n",
    "\n",
    "    # Drop null rows\n",
    "    data = data.dropna()\n",
    "\n",
    "    # Encode text columns to number values\n",
    "    for category in categorical:\n",
    "        le = LabelEncoder()\n",
    "        data[category] = le.fit_transform(data[category])\n",
    "        label_encoders[category] = le\n",
    "\n",
    "    for col in data.columns:\n",
    "        if(col not in categorical):\n",
    "            data[col] = (data[col].astype('float') - np.mean(data[col].astype('float')))/np.std(data[col].astype('float'))\n",
    "\n",
    "    # print(data.describe())\n",
    "    # print(data.describe(include='O'))\n",
    "\n",
    "    processed_data = data\n",
    "\n",
    "    # Get Training parameters\n",
    "    if dataset == \"German\":\n",
    "        target_col = data.columns[-1]\n",
    "        x = data.drop(columns=target_col, axis=1)\n",
    "        y = data[target_col].astype('int')\n",
    "    elif dataset == \"Australian\":\n",
    "        x = data.drop(14, axis=1)\n",
    "        y = data[14].astype('int')\n",
    "    elif dataset == \"Japanese\":\n",
    "        x = data.drop(15, axis=1)\n",
    "        y = data[15].astype('int')\n",
    "    elif dataset == \"Taiwan\":\n",
    "        x = data.drop('default_payment_next_month', axis=1)\n",
    "        y = data['default_payment_next_month'].astype('int')\n",
    "    elif dataset == \"Polish\":\n",
    "        x = data.drop('class', axis=1)\n",
    "        y = data['class'].astype('int')\n",
    "\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = test_size)\n",
    "    x_train = pd.DataFrame(x_train)\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    x_train = sc.fit_transform(x_train)\n",
    "    x_test = sc.transform(x_test)\n",
    "    \n",
    "    y_train = y_train[y_train.columns[0]].to_numpy()\n",
    "    y_test = y_test[y_test.columns[0]].to_numpy()\n",
    "\n",
    "    return (x_train, x_test, y_train, y_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocessing(\"German\", german, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7415b12f",
   "metadata": {},
   "source": [
    "## Build a local Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5aab5fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(t.nn.Module):\n",
    "    def __init__(self,input_channels,output_channels=2):\n",
    "        super(Net,self).__init__()\n",
    "        #Our network:\n",
    "        # Linear1->relu->Batchnorm->Linear2->relu->Batchnorm->Dropout->Linear3->output\n",
    "        # Softmax is added in the predict function\n",
    "        #This applies Linear transformation to input data. \n",
    "        self.fc1 = t.nn.Linear(input_channels,int(1.5*input_channels))\n",
    "        self.fc2 = t.nn.Linear(int(1.5*input_channels),int(1.5*input_channels))\n",
    "        self.fc3 = t.nn.Linear(int(1.5*input_channels),output_channels)\n",
    "        \n",
    "        self.relu = t.nn.ReLU()\n",
    "        self.dropout = t.nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = t.nn.BatchNorm1d(int(1.5*input_channels))\n",
    "        self.batchnorm2 = t.nn.BatchNorm1d(int(1.5*input_channels))\n",
    "        self.sigmoid = t.nn.Sigmoid()\n",
    "    #This must be implemented\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7b3e3b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List to store losses\n",
    "def classifier_train(epochs, model, optimizer, X,y,criterion):\n",
    "    losses = []\n",
    "    for i in range(epochs):\n",
    "        #Precit the output for Given input\n",
    "        y_pred = model.forward(X)\n",
    "        #Compute Cross entropy loss\n",
    "        loss = criterion(y_pred,y)\n",
    "        #Add loss to the list\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        #Print loss\n",
    "        if i%500==0:\n",
    "            print(\"Epoch:\",i,\" Loss:\",loss.item())\n",
    "        \n",
    "        #Clear the previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        #Compute gradients\n",
    "        loss.backward()\n",
    "        #Adjust weights\n",
    "        optimizer.step()\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9838a5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 0.7156881093978882\n",
      "Epoch: 500  Loss: 0.4959816634654999\n",
      "Epoch: 1000  Loss: 0.4516226649284363\n",
      "Epoch: 1500  Loss: 0.43293437361717224\n",
      "Epoch: 2000  Loss: 0.41597139835357666\n",
      "Epoch: 2500  Loss: 0.4253953695297241\n",
      "Epoch: 3000  Loss: 0.41753634810447693\n",
      "Epoch: 3500  Loss: 0.40889593958854675\n",
      "Epoch: 4000  Loss: 0.409296452999115\n",
      "Epoch: 4500  Loss: 0.3931117355823517\n"
     ]
    }
   ],
   "source": [
    "# Trnasform the input to tensor\n",
    "X_train_tensor = t.FloatTensor(X_train)\n",
    "y_train_tensor = t.tensor(y_train,dtype=t.long)\n",
    "\n",
    "# Define the input and output\n",
    "input_channels = X_train_tensor.shape[1]\n",
    "output_channels = 2\n",
    "\n",
    "#Initialize the model        \n",
    "model = Net(input_channels,output_channels)\n",
    "#Define loss criterion\n",
    "criterion = t.nn.CrossEntropyLoss()\n",
    "#Define the optimizer\n",
    "optimizer = t.optim.Adam(model.parameters(), lr=0.001)\n",
    "#Number of epochs\n",
    "epochs = 5000\n",
    "\n",
    "losses = classifier_train(\n",
    "    epochs, model, optimizer,X_train_tensor,y_train_tensor,criterion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac22890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
