{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccb753e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "import torch as t\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080121c6",
   "metadata": {},
   "source": [
    "## Launch a Duet Sever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1259c0ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤  ðŸŽ¸  â™ªâ™ªâ™ª Starting Duet â™«â™«â™«  ðŸŽ»  ðŸŽ¹\n",
      "\n",
      "â™«â™«â™« >\u001b[93m DISCLAIMER\u001b[0m: \u001b[1mDuet is an experimental feature currently in beta.\n",
      "â™«â™«â™« > Use at your own risk.\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "    > â¤ï¸ \u001b[91mLove\u001b[0m \u001b[92mDuet\u001b[0m? \u001b[93mPlease\u001b[0m \u001b[94mconsider\u001b[0m \u001b[95msupporting\u001b[0m \u001b[91mour\u001b[0m \u001b[93mcommunity!\u001b[0m\n",
      "    > https://github.com/sponsors/OpenMined\u001b[1m\n",
      "\n",
      "â™«â™«â™« > Punching through firewall to OpenGrid Network Node at:\n",
      "â™«â™«â™« > http://ec2-18-218-7-180.us-east-2.compute.amazonaws.com:5000\n",
      "â™«â™«â™« >\n",
      "â™«â™«â™« > ...waiting for response from OpenGrid Network... \n",
      "â™«â™«â™« > \u001b[92mDONE!\u001b[0m\n",
      "\n",
      "â™«â™«â™« > \u001b[95mSTEP 1:\u001b[0m Send the following code to your Duet Partner!\n",
      "\n",
      "import syft as sy\n",
      "duet = sy.join_duet(loopback=True)\n",
      "\n",
      "â™«â™«â™« > Connecting...\n",
      "\n",
      "â™«â™«â™« > \u001b[92mCONNECTED!\u001b[0m\n",
      "\n",
      "â™«â™«â™« > DUET LIVE STATUS  -  Objects: 0  Requests: 0   Messages: 2  Request Handlers: 0                                \r"
     ]
    }
   ],
   "source": [
    "duet = sy.launch_duet(loopback = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e094758",
   "metadata": {},
   "source": [
    "## Read Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10152986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "german = pd.read_csv(\"./data/german_data_1.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4c4829",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.20\n",
    "\n",
    "processed_data = None\n",
    "categorical = None\n",
    "label_encoders = {}\n",
    "\n",
    "def preprocessing(dataset, data, test_size):\n",
    "    \"\"\"\n",
    "    Preprocess dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: DataFrame\n",
    "        Pandas dataframe containing German dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    global processed_data\n",
    "    global categorical\n",
    "    global label_encoders\n",
    "\n",
    "    # Reset global variables\n",
    "    \n",
    "    processed_data = None\n",
    "    categorical = None\n",
    "    label_encoders = {}\n",
    "\n",
    "\n",
    "    if dataset == \"German\":\n",
    "        # Drop savings account and checkings account columns as they contain a lot\n",
    "        # of NaN values and may not always be available in real life scenarios\n",
    "        data = data.drop(columns = ['Saving accounts', 'Checking account'])\n",
    "        \n",
    "    dat_dict = data.to_dict()\n",
    "    new_dat_dict = {}\n",
    "\n",
    "    # rename columns(Make them lowercase and snakecase)\n",
    "    for key, value in dat_dict.items():\n",
    "        newKey = key\n",
    "        if type(key) == str:\n",
    "            newKey = newKey.lower().replace(' ', '_')\n",
    "        # if newKey != key:\n",
    "        new_dat_dict[newKey] = dat_dict[key]\n",
    "    del dat_dict\n",
    "\n",
    "    data = pd.DataFrame.from_dict(new_dat_dict)\n",
    "    del new_dat_dict\n",
    "\n",
    "\n",
    "    # print(data.describe())\n",
    "    # print(data.describe(include='O'))\n",
    "\n",
    "    cols = data.columns\n",
    "    num_cols = data._get_numeric_data().columns\n",
    "    categorical = list(set(cols) - set(num_cols))\n",
    "\n",
    "    # Drop null rows\n",
    "    data = data.dropna()\n",
    "\n",
    "    # Encode text columns to number values\n",
    "    for category in categorical:\n",
    "        le = LabelEncoder()\n",
    "        data[category] = le.fit_transform(data[category])\n",
    "        label_encoders[category] = le\n",
    "\n",
    "    for col in data.columns:\n",
    "        if(col not in categorical):\n",
    "            data[col] = (data[col].astype('float') - np.mean(data[col].astype('float')))/np.std(data[col].astype('float'))\n",
    "\n",
    "    # print(data.describe())\n",
    "    # print(data.describe(include='O'))\n",
    "\n",
    "    processed_data = data\n",
    "\n",
    "    # Get Training parameters\n",
    "    if dataset == \"German\":\n",
    "        target_col = data.columns[-1]\n",
    "        x = data.drop(columns=target_col, axis=1)\n",
    "        y = data[target_col].astype('int')\n",
    "    elif dataset == \"Australian\":\n",
    "        x = data.drop(14, axis=1)\n",
    "        y = data[14].astype('int')\n",
    "    elif dataset == \"Japanese\":\n",
    "        x = data.drop(15, axis=1)\n",
    "        y = data[15].astype('int')\n",
    "    elif dataset == \"Taiwan\":\n",
    "        x = data.drop('default_payment_next_month', axis=1)\n",
    "        y = data['default_payment_next_month'].astype('int')\n",
    "    elif dataset == \"Polish\":\n",
    "        x = data.drop('class', axis=1)\n",
    "        y = data['class'].astype('int')\n",
    "\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = test_size)\n",
    "    x_train = pd.DataFrame(x_train)\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    x_train = sc.fit_transform(x_train)\n",
    "    x_test = sc.transform(x_test)\n",
    "    \n",
    "    y_train = y_train[y_train.columns[0]].to_numpy()\n",
    "    y_test = y_test[y_test.columns[0]].to_numpy()\n",
    "\n",
    "    return (x_train, x_test, y_train, y_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocessing(\"German\", german, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd3565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trnasform the input to tensor\n",
    "X_train_tensor = t.FloatTensor(X_train)\n",
    "y_train_tensor = t.tensor(y_train,dtype=t.long)\n",
    "X_test_tensor = t.FloatTensor(X_test)\n",
    "y_test_tensor = t.tensor(y_test,dtype=t.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eea2ee",
   "metadata": {},
   "source": [
    "## Send data to Duet server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41defed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor.tag(\"X_Train_Owner1\")\n",
    "X_train_tensor.describe(\"Dataset of 400 samples, 7 feature\")\n",
    "X_train_ptr = X_train_tensor.send(duet,pointable = True)\n",
    "\n",
    "y_train_tensor.tag(\"y_Train_Owner1\")\n",
    "y_train_tensor.describe(\"Dataset of 400 samples, 1 feature\")\n",
    "y_train_ptr = y_train_tensor.send(duet,pointable = True)\n",
    "\n",
    "X_test_tensor.tag(\"X_Test_Owner1\")\n",
    "X_test_tensor.describe(\"Dataset of 400 samples, 7 feature\")\n",
    "X_test_ptr = X_test_tensor.send(duet,pointable = True)\n",
    "\n",
    "y_test_tensor.tag(\"y_Test_Owner1\")\n",
    "y_test_tensor.describe(\"Dataset of 400 samples, 1 feature\")\n",
    "y_test_ptr = y_test_tensor.send(duet,pointable = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1a3ca6",
   "metadata": {},
   "source": [
    "## Create a request handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0933484",
   "metadata": {},
   "outputs": [],
   "source": [
    "duet.requests.add_handler(\n",
    "    action=\"accept\",\n",
    "    print_local=True,  # print the result in your notebook\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795d441d",
   "metadata": {},
   "source": [
    "## Build a local Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a6f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(t.nn.Module):\n",
    "    def __init__(self,input_channels,output_channels):\n",
    "        super(Net,self).__init__()\n",
    "        #Our network:\n",
    "        # Linear1->relu->Batchnorm->Linear2->relu->Batchnorm->Dropout->Linear3->output\n",
    "        # Softmax is added in the predict function\n",
    "        #This applies Linear transformation to input data. \n",
    "        self.fc1 = t.nn.Linear(input_channels,int(1.5*input_channels))\n",
    "        self.fc2 = t.nn.Linear(int(1.5*input_channels),int(1.5*input_channels))\n",
    "        self.fc3 = t.nn.Linear(int(1.5*input_channels),output_channels)\n",
    "        \n",
    "        self.relu = t.nn.ReLU()\n",
    "        self.dropout = t.nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = t.nn.BatchNorm1d(int(1.5*input_channels))\n",
    "        self.batchnorm2 = t.nn.BatchNorm1d(int(1.5*input_channels))\n",
    "        self.sigmoid = t.nn.Sigmoid()\n",
    "    #This must be implemented\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    def predict(self,x):\n",
    "        output = self.forward(x)\n",
    "        prediction = t.argmax(output,1)\n",
    "        return prediction\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeef040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_train(epochs, model, optimizer, X,y,criterion):\n",
    "    losses = []\n",
    "    for i in range(epochs):\n",
    "        #Precit the output for Given input\n",
    "        y_pred = model.forward(X)\n",
    "        #Compute Cross entropy loss\n",
    "        loss = criterion(y_pred,y)\n",
    "        #Add loss to the list\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        #Print loss\n",
    "        if i%500==0:\n",
    "            print(\"Epoch:\",i,\" Loss:\",loss.item())\n",
    "        \n",
    "        #Clear the previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        #Compute gradients\n",
    "        loss.backward()\n",
    "        #Adjust weights\n",
    "        optimizer.step()\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea472b34",
   "metadata": {},
   "source": [
    "## Local Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9622f283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and output\n",
    "input_channels = X_train_tensor.shape[1]\n",
    "output_channels = 2\n",
    "\n",
    "#Initialize the model        \n",
    "model = Net(input_channels,output_channels)\n",
    "#Define loss criterion\n",
    "criterion = t.nn.CrossEntropyLoss()\n",
    "#Define the optimizer\n",
    "optimizer = t.optim.Adam(model.parameters(), lr=0.001)\n",
    "#Number of epochs\n",
    "epochs = 5000\n",
    "\n",
    "losses = classifier_train(\n",
    "    epochs, model, optimizer,X_train_tensor,y_train_tensor,criterion\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7154ba09",
   "metadata": {},
   "source": [
    "## Accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a2ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def accuracy(model,X,y):\n",
    "    print(accuracy_score(model.predict(X),y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0625eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(model,X_test_tensor,y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408d6728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
